{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c513eebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaba1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fastapi',\n",
       " 'langchain',\n",
       " 'pydantic',\n",
       " 'numpy',\n",
       " 'pandas',\n",
       " 'requests',\n",
       " 'flask',\n",
       " 'django',\n",
       " 'matplotlib']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "libs = {\n",
    "    \"fastapi\": \"https://context7.com/tiangolo/fastapi\",\n",
    "    \"langchain\": \"https://context7.com/context7/python_langchain_com-docs-introduction\",\n",
    "    \"pydantic\": \"https://context7.com/context7/pydantic_dev\",\n",
    "    \"numpy\": \"https://context7.com/numpy/numpy\",\n",
    "    \"pandas\": \"https://context7.com/pandas-dev/pandas\",\n",
    "    \"requests\": \"https://context7.com/psf/requests\",\n",
    "    \"flask\": \"https://context7.com/pallets/flask\",\n",
    "    \"django\": \"https://context7.com/django/django\",\n",
    "    \"matplotlib\": \"https://context7.com/matplotlib/matplotlib\"\n",
    "}\n",
    "\n",
    "private_libs = [\n",
    "    \"alis_lib\"\n",
    "]\n",
    "\n",
    "lib_names = list(libs.keys())\n",
    "lib_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61a2c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "\n",
    "# llm = init_chat_model(\"gpt-4.1-nano\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6b95481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import structured_outputs\n",
    "from structured_outputs import Lib\n",
    "\n",
    "importlib.reload(structured_outputs)\n",
    "\n",
    "lib_extractor_llm = llm.with_structured_output(Lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58e25100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Private repos: \n",
      "{'project_demo': ' calculate dot product of two vectors'}\n",
      "public repos: \n",
      "{'numpy': ' perform vector and matrix calculations'}\n"
     ]
    }
   ],
   "source": [
    "code = \"\"\"\n",
    "import project_demo\n",
    "import numpy\n",
    "\n",
    "use project demo to calculate the dot product of 2 vectors\n",
    "\"\"\"\n",
    "\n",
    "ls = lib_extractor_llm.invoke(code)\n",
    "\n",
    "print(\"Private repos: \")\n",
    "print(ls.to_dict_private())\n",
    "\n",
    "print(\"public repos: \")\n",
    "print(ls.to_dict_public())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef999d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pydantic import BaseModel, Field\n",
    "# from typing import List\n",
    "\n",
    "# class Lib(BaseModel):\n",
    "#     f\"\"\"\n",
    "#         You will receive a user prompt that may or may not contain code. Your task is to:\n",
    "#         If the prompt contains code:\n",
    "#         Analyze the code to extract any function calls or relevant usage patterns.\n",
    "#         Identify which Python libraries are likely being used or would be helpful to solve the problem.\n",
    "#         If the prompt contains only natural language (no code):\n",
    "#         Infer what Python libraries might help solve the user's task, based on keywords or task intent (e.g., web scraping, data analysis, NLP, etc.).\n",
    "#         Return a list of all relevant library names. Do not return duplicates.\n",
    "#         The first word should in each entry should be the name of the library, and then there should be a small sentence 7 to 15 wrods at max indicating what are the things that i need from this library,\n",
    "#         avoid using questions, if you need more that one thing insert this library with difference search sentence, i will search a vector database so write a seach sentence that workd best with vector databases\n",
    "#         If the prompt is not code-related and has no technical request, return an empty list.\n",
    "#         Suggest all libraries that are present in the code, if code is available\n",
    "        \n",
    "#         Constraints:\n",
    "#         Don't the same library twice in the code unless its really needed for a different thing, not a thing similar to a previous topic\n",
    "#         Be accurate — only suggest libraries that are truly applicable.\n",
    "#         Do not hallucinate libraries unless their usage is clearly implied.\n",
    "#         Include libraries even if the user doesn't mention them by name, but their functionality is required.\n",
    "#     \"\"\"\n",
    "\n",
    "#     lib: List[str] = Field(\n",
    "#         description=(\n",
    "#             \"List of identified libraries from the user prompt or history. \"\n",
    "#             \"Each item must begin with the library name followed by a clear, concise task expressed as a direct command. \"\n",
    "#             \"Use action verbs (e.g., 'build', 'visualize', 'compute') to start each task description — this improves relevance in vector search. \"\n",
    "#             \"Avoid vague or filler phrases like 'implement endpoint for numerical computations with advanced math and visualization'. \"\n",
    "#             \"Be specific about what you want the library to do. \"\n",
    "#             \"No clutter — treat each description as if querying a vector database. \"\n",
    "#             \"Always separate concerns, don't use FastAPI  .... for numerical calculations (ommit thes ething only focus on the purpose of the library) VERY IMPORTANT.\" \\\n",
    "#             \"Always use the following formating `lib_name: seach sentence`.\"\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     def to_dict(self) -> dict:\n",
    "        \n",
    "#         out = {}\n",
    "#         for item in self.lib:\n",
    "#             name, search = item.split(':')\n",
    "#             out[name.lower()] = search\n",
    "\n",
    "#         filtered = {k: v for k, v in out.items() if k in lib_names}\n",
    "\n",
    "#         return filtered\n",
    "\n",
    "# class CodeTextSep(BaseModel):\n",
    "#     \"\"\"\n",
    "#         You will be given a prompt that contains a mix of normal text and code.\n",
    "#         Your task is not to analyze, modify, or interpret either part.\n",
    "#         Simply separate the input into two distinct parts:\n",
    "#         Text Section: This includes any explanatory text, instructions, or natural language written by the user.\n",
    "#         Code Section: This includes only the code, exactly as written. Do not alter, reformat, or fix the code in any way.\n",
    "#     \"\"\"\n",
    "\n",
    "#     text: str = Field(description=\"This is the text part of the promt\")\n",
    "#     code: str = Field(description=\"This is the code part of the promt\")\n",
    "\n",
    "# lib_extractor_llm = llm.with_structured_output(Lib)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63c7c12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lib(lib=['alis_lib: Create a new financial document.'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code = \"\"\"\n",
    "import alis_lib\n",
    "\n",
    "using alis_lib create a new finantial document\n",
    "\"\"\"\n",
    "\n",
    "lib_extractor_llm.invoke(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acaa1cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'requests': ' make API requests to external services'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
    "\n",
    "tools_registry = {\n",
    "    \"scrap_docs\": scrap_docs\n",
    "}\n",
    "\n",
    "tools = [scrap_docs]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "query = \"Write a function that chains 2 llms using langchain\"\n",
    "messages = [\n",
    "    SystemMessage(\"you are a very funny and helpful assistant, joke with the user and put silly funny comments in the code\"),\n",
    "    HumanMessage(query),\n",
    "]\n",
    "\n",
    "ai_message = llm_with_tools.invoke(query)\n",
    "\n",
    "messages.append(ai_message)\n",
    "\n",
    "tool_calls = ai_message.tool_calls\n",
    "\n",
    "for tool_call in tool_calls:\n",
    "    selected_tool = tools_registry[tool_call['name']]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "final_output = llm_with_tools.invoke(messages)\n",
    "\n",
    "the above is the given code, i want it to be in a way that is reusable in code using things in langchain, i also want to do api requests\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "lib_ext_output = lib_extractor_llm.invoke(query)\n",
    "lib_dict = lib_ext_output.to_dict()\n",
    "\n",
    "lib_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5dbc565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def scrap_docs(lib_name: str, topic: str) -> str:\n",
    "    \"\"\"\n",
    "        Uses get_docs function to scrap documentation from context7, if code is provided\n",
    "        use this code to reason about what the user want, the topic should be something related to the code\n",
    "        and the prompt provided by the user, if there is code try reason what is the library used to scrap\n",
    "        its docs using the get_docs function be more descriptive with the topic focus on making it relavant to what the user is promting you.\n",
    "        You can use this tool multiple times.\n",
    "\n",
    "        Args:\n",
    "            lib_name: name of the library that we want to scrap that is provided by context7\n",
    "            topic: topic of interets in the library to get more accurate and useful docs for out use case\n",
    "    \"\"\"\n",
    "\n",
    "    return get_docs(libs[lib_name], topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a033a33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['another_feeling', 'code'], input_types={}, partial_variables={}, template='Improve the following code and add descriptive comments:\\n\\n{code}, make it funny, make it {another_feeling}.')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"Improve the following code and add descriptive comments:\\n\\n{code}\")\n",
    "\n",
    "prompt = (\n",
    "    PromptTemplate.from_template(\"Improve the following code and add descriptive comments:\\n\\n{code}\") + \n",
    "    \", make it funny\" +\n",
    "    \", make it {another_feeling}.\"\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8a0c266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
    "\n",
    "def execute_llm(llm, query: str, system_promt: str):\n",
    "\n",
    "    tools_registry = {\n",
    "        \"scrap_docs\": scrap_docs\n",
    "    }\n",
    "\n",
    "    tools = [scrap_docs]\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(system_promt),\n",
    "        HumanMessage(query),\n",
    "    ]\n",
    "\n",
    "    useful_libs = lib_extractor_llm.invoke(query)\n",
    "    useful_libs = useful_libs.to_dict()\n",
    "\n",
    "    libs_text = \"\"\n",
    "\n",
    "    for k, v in useful_libs.items():\n",
    "        libs_text += f\"{k} search for `{v}`\\n\"\n",
    "\n",
    "    next_prompt = f\"get the docs and search for the topics of the following libraries\\n{libs_text}\"\n",
    "\n",
    "    ai_message = llm_with_tools.invoke(next_prompt)\n",
    "    messages.append(ai_message)\n",
    "\n",
    "    tool_calls = ai_message.tool_calls\n",
    "\n",
    "    for tool_call in tool_calls:\n",
    "        selected_tool = tools_registry[tool_call['name']]\n",
    "        tool_msg = selected_tool.invoke(tool_call)\n",
    "        messages.append(tool_msg)\n",
    "\n",
    "    output = llm_with_tools.invoke(messages)\n",
    "    return (output, messages)\n",
    "\n",
    "system_prompt = \"You are a very helpful AI coding assistance that write code from the prompt given, you focus on code everything else don't give it attention\"\n",
    "query = \"\"\"\n",
    "\n",
    "    from langchain.chat_models import init_chat_model\n",
    "\n",
    "    llm = init_chat_model(\"gpt-4.1\", model_provider=\"openai\")\n",
    "\n",
    "    def execute_llm(llm, query: str, system_promt: str):\n",
    "\n",
    "        tools_registry = {\n",
    "            \"scrap_docs\": scrap_docs\n",
    "        }\n",
    "\n",
    "        tools = [scrap_docs]\n",
    "        llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(system_promt),\n",
    "            HumanMessage(query),\n",
    "        ]\n",
    "\n",
    "        useful_libs = lib_extractor_llm.invoke(query)\n",
    "        useful_libs = useful_libs.to_dict()\n",
    "\n",
    "        libs_text = \"\"\n",
    "\n",
    "        for k, v in useful_libs.items():\n",
    "            libs_text += f\"{k} search for `{v}`\\n\"\n",
    "\n",
    "        next_prompt = f\"get the docs and search for the topics of the following libraries\\n{libs_text}\"\n",
    "\n",
    "        ai_message = llm_with_tools.invoke(next_prompt)\n",
    "        messages.append(ai_message)\n",
    "\n",
    "        tool_calls = ai_message.tool_calls\n",
    "\n",
    "        for tool_call in tool_calls:\n",
    "            selected_tool = tools_registry[tool_call['name']]\n",
    "            tool_msg = selected_tool.invoke(tool_call)\n",
    "            messages.append(tool_msg)\n",
    "\n",
    "        output = llm_with_tools.invoke(messages)\n",
    "        return (output, messages)\n",
    "\n",
    "    create a simple fastapi app and wrap this code with a post request so that i can send a query and a system prompt from postman \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "output, messages = execute_llm(llm, query, system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eee2720e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"You are a very helpful AI coding assistance that write code from the prompt given, you focus on code everything else don't give it attention\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='\\n\\n    from langchain.chat_models import init_chat_model\\n\\n    llm = init_chat_model(\"gpt-4.1\", model_provider=\"openai\")\\n\\n    def execute_llm(llm, query: str, system_promt: str):\\n\\n        tools_registry = {\\n            \"scrap_docs\": scrap_docs\\n        }\\n\\n        tools = [scrap_docs]\\n        llm_with_tools = llm.bind_tools(tools)\\n\\n        messages = [\\n            SystemMessage(system_promt),\\n            HumanMessage(query),\\n        ]\\n\\n        useful_libs = lib_extractor_llm.invoke(query)\\n        useful_libs = useful_libs.to_dict()\\n\\n        libs_text = \"\"\\n\\n        for k, v in useful_libs.items():\\n            libs_text += f\"{k} search for `{v}`\\n\"\\n\\n        next_prompt = f\"get the docs and search for the topics of the following libraries\\n{libs_text}\"\\n\\n        ai_message = llm_with_tools.invoke(next_prompt)\\n        messages.append(ai_message)\\n\\n        tool_calls = ai_message.tool_calls\\n\\n        for tool_call in tool_calls:\\n            selected_tool = tools_registry[tool_call[\\'name\\']]\\n            tool_msg = selected_tool.invoke(tool_call)\\n            messages.append(tool_msg)\\n\\n        output = llm_with_tools.invoke(messages)\\n        return (output, messages)\\n\\n    create a simple fastapi app and wrap this code with a post request so that i can send a query and a system prompt from postman \\n    \\n', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_KEcZxruChbSuTeqZ2KbZUrfe', 'function': {'arguments': '{\"lib_name\": \"fastapi\", \"topic\": \"create RESTful endpoints\"}', 'name': 'scrap_docs'}, 'type': 'function'}, {'id': 'call_IvOZHecpORfY9Z5kINTPfRaw', 'function': {'arguments': '{\"lib_name\": \"pydantic\", \"topic\": \"validate POST request payloads\"}', 'name': 'scrap_docs'}, 'type': 'function'}, {'id': 'call_JDYofLPq4oCpcfW1UupJelJ9', 'function': {'arguments': '{\"lib_name\": \"langchain\", \"topic\": \"utilize models for processing\"}', 'name': 'scrap_docs'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 228, 'total_tokens': 319, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': None, 'id': 'chatcmpl-BtyB3oFDFsKBTZe7YSsvPqcvXfhOO', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--b5edb583-c858-41f6-a0da-3ab0252ed447-0', tool_calls=[{'name': 'scrap_docs', 'args': {'lib_name': 'fastapi', 'topic': 'create RESTful endpoints'}, 'id': 'call_KEcZxruChbSuTeqZ2KbZUrfe', 'type': 'tool_call'}, {'name': 'scrap_docs', 'args': {'lib_name': 'pydantic', 'topic': 'validate POST request payloads'}, 'id': 'call_IvOZHecpORfY9Z5kINTPfRaw', 'type': 'tool_call'}, {'name': 'scrap_docs', 'args': {'lib_name': 'langchain', 'topic': 'utilize models for processing'}, 'id': 'call_JDYofLPq4oCpcfW1UupJelJ9', 'type': 'tool_call'}], usage_metadata={'input_tokens': 228, 'output_tokens': 91, 'total_tokens': 319, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " ToolMessage(content='TITLE: Create Basic API with FastAPI\\nDESCRIPTION: Create a simple FastAPI application with two endpoints: a root endpoint that returns a greeting and an `/items/{item_id}` endpoint that returns the item ID and an optional query parameter.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/zh-hant/docs/index.md#_snippet_2\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom typing import Union\\n\\nfrom fastapi import FastAPI\\n\\napp = FastAPI()\\n\\n\\n@app.get(\"/\")\\ndef read_root():\\n    return {\"Hello\": \"World\"}\\n\\n\\n@app.get(\"/items/{item_id}\")\\ndef read_item(item_id: int, q: Union[str, None] = None):\\n    return {\"item_id\": item_id, \"q\": q}\\n```\\n\\n----------------------------------------\\n\\nTITLE: Creating a Basic FastAPI Application\\nDESCRIPTION: This code creates a basic FastAPI application with two endpoints: a root endpoint that returns a simple JSON response and an items endpoint that accepts an item ID and an optional query parameter.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/bn/docs/index.md#_snippet_2\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom typing import Union\\n\\nfrom fastapi import FastAPI\\n\\napp = FastAPI()\\n\\n\\n@app.get(\"/\")\\ndef read_root():\\n    return {\"Hello\": \"World\"}\\n\\n\\n@app.get(\"/items/{item_id}\")\\ndef read_item(item_id: int, q: Union[str, None] = None):\\n    return {\"item_id\": item_id, \"q\": q}\\n```\\n\\n----------------------------------------\\n\\nTITLE: Creating WebSocket endpoint\\nDESCRIPTION: Creates a WebSocket endpoint in a FastAPI application. It shows how to define a WebSocket route using the @app.websocket decorator.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/ru/docs/advanced/websockets.md#_snippet_1\\n\\nLANGUAGE: Python\\nCODE:\\n```\\n@app.websocket(\"/ws\")\\nasync def websocket_endpoint(websocket: WebSocket):\\n    ...\\n```\\n\\n----------------------------------------\\n\\nTITLE: FastAPI Endpoint: Create a New Hero (POST /heroes/)\\nDESCRIPTION: Implements a POST endpoint to create a new `Hero` entry in the database. It receives hero data from the request body, adds it to the session, commits the transaction, and returns the newly created hero with its database-assigned ID.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/zh/docs/tutorial/sql-databases.md#_snippet_6\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom fastapi import APIRouter\\nfrom sqlmodel import Session\\n\\n# Assuming Hero model and SessionDep are defined elsewhere\\n# from .models import Hero\\n# from .dependencies import SessionDep\\n\\nrouter = APIRouter()\\n\\n@router.post(\"/heroes/\", response_model=Hero)\\ndef create_hero(hero: Hero, session: SessionDep):\\n    session.add(hero)\\n    session.commit()\\n    session.refresh(hero)\\n    return hero\\n```\\n\\n----------------------------------------\\n\\nTITLE: Creating a basic FastAPI application\\nDESCRIPTION: This Python code creates a basic FastAPI application with two endpoints: a root endpoint (\"/\") that returns a greeting and an endpoint for retrieving items by ID (\"/items/{item_id}\"). It uses type hints and Pydantic for data validation.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/uk/docs/index.md#_snippet_2\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom typing import Union\\n\\nfrom fastapi import FastAPI\\n\\napp = FastAPI()\\n\\n\\n@app.get(\"/\")\\ndef read_root():\\n    return {\"Hello\": \"World\"}\\n\\n\\n@app.get(\"/items/{item_id}\")\\ndef read_item(item_id: int, q: Union[str, None] = None):\\n    return {\"item_id\": item_id, \"q\": q}\\n```\\n\\n----------------------------------------\\n\\nTITLE: Creating a basic FastAPI application\\nDESCRIPTION: This Python code defines a simple FastAPI application with two endpoints: a root endpoint that returns a greeting and an /items/{item_id} endpoint that returns the item ID and an optional query parameter.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/fr/docs/deployment/docker.md#_snippet_3\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom typing import Optional\\n\\nfrom fastapi import FastAPI\\n\\napp = FastAPI()\\n\\n\\n@app.get(\"/\")\\ndef read_root():\\n    return {\"Hello\": \"World\"}\\n\\n\\n@app.get(\"/items/{item_id}\")\\ndef read_item(item_id: int, q: Optional[str] = None):\\n    return {\"item_id\": item_id, \"q\": q}\\n```\\n\\n----------------------------------------\\n\\nTITLE: Creating an async FastAPI application\\nDESCRIPTION: This Python code demonstrates how to create a basic FastAPI application using `async def` for asynchronous request handling. It includes two endpoints: a root endpoint and an endpoint for retrieving items by ID, both defined as asynchronous functions.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/uk/docs/index.md#_snippet_3\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom typing import Union\\n\\nfrom fastapi import FastAPI\\n\\napp = FastAPI()\\n\\n\\n@app.get(\"/\")\\nasync def read_root():\\n    return {\"Hello\": \"World\"}\\n\\n\\n@app.get(\"/items/{item_id}\")\\nasync def read_item(item_id: int, q: Union[str, None] = None):\\n    return {\"item_id\": item_id, \"q\": q}\\n```\\n\\n----------------------------------------\\n\\nTITLE: Creating a WebSocket route\\nDESCRIPTION: Creates a WebSocket endpoint in a FastAPI application using the `@app.websocket()` decorator.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/pt/docs/advanced/websockets.md#_snippet_3\\n\\nLANGUAGE: Python\\nCODE:\\n```\\n@app.websocket(\"/ws\")\\nasync def websocket_endpoint(websocket: WebSocket):\\n    ...\\n```\\n\\n----------------------------------------\\n\\nTITLE: Creating a basic FastAPI application\\nDESCRIPTION: This Python code defines a simple FastAPI application with two endpoints: `/` which returns a greeting, and `/items/{item_id}` which returns the item ID and an optional query parameter. It imports FastAPI, creates an app instance, and defines the endpoints using decorators.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/pl/docs/index.md#_snippet_2\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom typing import Union\\n\\nfrom fastapi import FastAPI\\n\\napp = FastAPI()\\n\\n\\n@app.get(\"/\")\\ndef read_root():\\n    return {\"Hello\": \"World\"}\\n\\n\\n@app.get(\"/items/{item_id}\")\\ndef read_item(item_id: int, q: Union[str, None] = None):\\n    return {\"item_id\": item_id, \"q\": q}\\n```\\n\\n----------------------------------------\\n\\nTITLE: Creating a WebSocket endpoint in FastAPI\\nDESCRIPTION: This snippet demonstrates how to create a WebSocket endpoint in a FastAPI application using the WebSocket class. It defines a route that handles WebSocket connections, receives messages from the client, and sends back a response.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/de/docs/advanced/websockets.md#_snippet_0\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom fastapi import FastAPI, WebSocket\\n\\napp = FastAPI()\\n\\n@app.websocket(\"/ws\")\\nasync def websocket_endpoint(websocket: WebSocket):\\n    await websocket.accept()\\n    while True:\\n        data = await websocket.receive_text()\\n        await websocket.send_text(f\"Message text was: {data}\")\\n```\\n\\n----------------------------------------\\n\\nTITLE: Creating an async FastAPI application\\nDESCRIPTION: This Python code defines a simple FastAPI application with two asynchronous endpoints: `/` which returns a greeting, and `/items/{item_id}` which returns the item ID and an optional query parameter. It imports FastAPI, creates an app instance, and defines the endpoints using `async def`.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/pl/docs/index.md#_snippet_3\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom typing import Union\\n\\nfrom fastapi import FastAPI\\n\\napp = FastAPI()\\n\\n\\n@app.get(\"/\")\\nasync def read_root():\\n    return {\"Hello\": \"World\"}\\n\\n\\n@app.get(\"/items/{item_id}\")\\nasync def read_item(item_id: int, q: Union[str, None] = None):\\n    return {\"item_id\": item_id, \"q\": q}\\n```\\n\\n----------------------------------------\\n\\nTITLE: Simple FastAPI Application\\nDESCRIPTION: Creates a basic FastAPI application with a single endpoint that returns a JSON response.  It requires the fastapi library to be installed. The endpoint is accessed at the root path (\\'/\\') and returns a JSON object with a \\'message\\' key.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/ko/docs/tutorial/first-steps.md#_snippet_0\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom fastapi import FastAPI\\n\\napp = FastAPI()\\n\\n\\n@app.get(\"/\")\\nasync def root():\\n    return {\"message\": \"Hello World\"}\\n\\n```\\n\\n----------------------------------------\\n\\nTITLE: Create a Path Operation for Testing in FastAPI\\nDESCRIPTION: This snippet creates a simple path operation to test if the documentation is working correctly. It defines a GET endpoint `/test` that returns a dictionary with a message.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/es/docs/how-to/custom-docs-ui-assets.md#_snippet_2\\n\\nLANGUAGE: Python\\nCODE:\\n```\\n@app.get(\"/test\")\\nasync def test():\\n    return {\"message\": \"Hello World\"}\\n```\\n\\n----------------------------------------\\n\\nTITLE: Create a Basic FastAPI Application\\nDESCRIPTION: This Python code defines a simple FastAPI application with two endpoints: a root endpoint returning \\'Hello World\\' and an item endpoint demonstrating path and query parameters. It serves as the core application logic to be containerized.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/en/docs/deployment/docker.md#_snippet_3\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom typing import Union\\n\\nfrom fastapi import FastAPI\\n\\napp = FastAPI()\\n\\n\\n@app.get(\"/\")\\ndef read_root():\\n    return {\"Hello\": \"World\"}\\n\\n\\n@app.get(\"/items/{item_id}\")\\ndef read_item(item_id: int, q: Union[str, None] = None):\\n    return {\"item_id\": item_id, \"q\": q}\\n```\\n\\n----------------------------------------\\n\\nTITLE: FastAPI Application with Tagged Endpoints\\nDESCRIPTION: A FastAPI application demonstrating the use of tags to group related endpoints. This setup allows client generators to create separate service classes (e.g., `ItemsService`, `UsersService`) based on these tags, improving client code organization.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/em/docs/advanced/generate-clients.md#_snippet_4\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom typing import Union\\n\\nfrom fastapi import FastAPI, APIRouter\\nfrom pydantic import BaseModel\\n\\n\\nclass Item(BaseModel):\\n    name: str\\n    price: float\\n    is_offer: Union[bool, None] = None\\n\\n\\nclass User(BaseModel):\\n    username: str\\n    email: Union[str, None] = None\\n\\n\\napp = FastAPI()\\n\\nitems_router = APIRouter(tags=[\"items\"])\\nusers_router = APIRouter(tags=[\"users\"])\\n\\n\\n@items_router.get(\"/items/{item_id}\")\\ndef read_item(item_id: int, q: Union[str, None] = None):\\n    return {\"item_id\": item_id, \"q\": q}\\n\\n\\n\\n```\\n\\n----------------------------------------\\n\\nTITLE: Create a Route Operation for Testing in FastAPI\\nDESCRIPTION: This snippet creates a simple route operation to test if the custom documentation setup is working correctly. It defines a GET endpoint at the root path that returns a JSON response.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/pt/docs/how-to/custom-docs-ui-assets.md#_snippet_2\\n\\nLANGUAGE: Python\\nCODE:\\n```\\n@app.get(\"/\")\\nasync def read_root():\\n    return {\"Hello\": \"World\"}\\n```\\n\\n----------------------------------------\\n\\nTITLE: Simple FastAPI Application\\nDESCRIPTION: This code defines a basic FastAPI application with a single endpoint that returns a JSON response. It uses the FastAPI framework to create an API endpoint at the root path (\\'/\\') that returns a JSON object with a \\'message\\' key.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/de/docs/tutorial/first-steps.md#_snippet_0\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom fastapi import FastAPI\\n\\napp = FastAPI()\\n\\n\\n@app.get(\"/\")\\nasync def root():\\n    return {\"message\": \"Hello World\"}\\n\\n```\\n\\n----------------------------------------\\n\\nTITLE: Initializing FastAPI App with Basic Endpoints\\nDESCRIPTION: Creates a FastAPI application instance and defines two GET endpoints: one for the root path (\\'/\\') and another for \\'/items/{item_id}\\' with a path parameter and an optional query parameter. It uses the FastAPI library and returns JSON responses.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/de/docs/index.md#_snippet_2\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom typing import Union\\n\\nfrom fastapi import FastAPI\\n\\napp = FastAPI()\\n\\n\\n@app.get(\"/\")\\ndef read_root():\\n    return {\"Hello\": \"World\"}\\n\\n\\n@app.get(\"/items/{item_id}\")\\ndef read_item(item_id: int, q: Union[str, None] = None):\\n    return {\"item_id\": item_id, \"q\": q}\\n```\\n\\n----------------------------------------\\n\\nTITLE: Initializing FastAPI App with Async Endpoints\\nDESCRIPTION: Creates a FastAPI application instance and defines two asynchronous GET endpoints: one for the root path (\\'/\\') and another for \\'/items/{item_id}\\' with a path parameter and an optional query parameter. It uses the FastAPI library and returns JSON responses using async def.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/de/docs/index.md#_snippet_3\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom typing import Union\\n\\nfrom fastapi import FastAPI\\n\\napp = FastAPI()\\n\\n\\n@app.get(\"/\")\\nasync def read_root():\\n    return {\"Hello\": \"World\"}\\n\\n\\n@app.get(\"/items/{item_id}\")\\nasync def read_item(item_id: int, q: Union[str, None] = None):\\n    return {\"item_id\": item_id, \"q\": q}\\n```\\n\\n----------------------------------------\\n\\nTITLE: Add a Basic FastAPI Endpoint for Testing\\nDESCRIPTION: This simple path operation defines a root endpoint (`/`) that returns a basic JSON message. It serves as a minimal functional endpoint to verify that the FastAPI application is running correctly, especially after making changes to the documentation UI configuration or static file serving.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/en/docs/how-to/custom-docs-ui-assets.md#_snippet_2\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom fastapi import FastAPI\\n\\napp = FastAPI() # Assuming app is already defined or this is a minimal example\\n\\n@app.get(\"/\")\\nasync def read_root():\\n    return {\"message\": \"Hello World\"}\\n```\\n\\n----------------------------------------\\n\\nTITLE: Defining FastAPI Path Operations with Decorators\\nDESCRIPTION: Illustrates how to define API endpoints in FastAPI using path operation decorators. Each decorator (e.g., `@app.get`, `@app.post`) maps an HTTP method and a URL path to a Python asynchronous function, handling incoming requests.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/en/docs/tutorial/first-steps.md#_snippet_6\\n\\nLANGUAGE: Python\\nCODE:\\n```\\n# Example of a GET path operation\\n@app.get(\"/\")\\nasync def read_root():\\n    \"\"\"\\n    Handles GET requests to the root path.\\n    Returns a simple JSON message.\\n    \"\"\"\\n    return {\"message\": \"Hello World\"}\\n\\n# Other common HTTP method decorators:\\n# @app.post(\"/items/\")  # For creating data\\n# @app.put(\"/items/{item_id}\") # For updating data\\n# @app.delete(\"/items/{item_id}\") # For deleting data\\n\\n# More exotic HTTP method decorators:\\n# @app.options(\"/options/\")\\n# @app.head(\"/head/\")\\n# @app.patch(\"/patch/\")\\n# @app.trace(\"/trace/\")\\n```\\n\\n----------------------------------------\\n\\nTITLE: FastAPI WebSocket Endpoint\\nDESCRIPTION: Creates a WebSocket endpoint in a FastAPI application. It accepts a WebSocket connection, receives messages, and sends back a response.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/pt/docs/advanced/websockets.md#_snippet_2\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom fastapi import FastAPI, WebSocket\\n\\napp = FastAPI()\\n\\n@app.websocket(\"/ws\")\\nasync def websocket_endpoint(websocket: WebSocket):\\n    await websocket.accept()\\n    while True:\\n        data = await websocket.receive_text()\\n        await websocket.send_text(f\"Message text was: {data}\")\\n```\\n\\n----------------------------------------\\n\\nTITLE: Define Basic FastAPI Application Endpoints\\nDESCRIPTION: Example Python code for a simple FastAPI application defining a root endpoint and an item endpoint with path and query parameters. Includes both synchronous and asynchronous function definitions for handling requests.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/zh/docs/index.md#_snippet_1\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom typing import Union\\n\\nfrom fastapi import FastAPI\\n\\napp = FastAPI()\\n\\n\\n@app.get(\"/\")\\ndef read_root():\\n    return {\"Hello\": \"World\"}\\n\\n\\n@app.get(\"/items/{item_id}\")\\ndef read_item(item_id: int, q: Union[str, None] = None):\\n    return {\"item_id\": item_id, \"q\": q}\\n```\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom typing import Union\\n\\nfrom fastapi import FastAPI\\n\\napp = FastAPI()\\n\\n\\n@app.get(\"/\")\\nasync def read_root():\\n    return {\"Hello\": \"World\"}\\n\\n\\n@app.get(\"/items/{item_id}\")\\nasync def read_item(item_id: int, q: Union[str, None] = None):\\n    return {\"item_id\": item_id, \"q\": q}\\n```\\n\\n----------------------------------------\\n\\nTITLE: Creating a FastAPI Instance\\nDESCRIPTION: This code snippet shows how to create an instance of the FastAPI class. This instance is used to define the API endpoints.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/ja/docs/tutorial/first-steps.md#_snippet_2\\n\\nLANGUAGE: python\\nCODE:\\n```\\napp = FastAPI()\\n```\\n\\n----------------------------------------\\n\\nTITLE: Add Custom Documentation Endpoints in FastAPI\\nDESCRIPTION: This snippet shows how to add custom documentation endpoints for Swagger UI and ReDoc, specifying custom CDN URLs for JavaScript and CSS files. It reuses FastAPI\\'s internal functions to create the HTML pages for the documentation, passing the necessary arguments such as openapi_url, title, and the custom CDN URLs.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/de/docs/how-to/custom-docs-ui-assets.md#_snippet_1\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom fastapi.openapi.docs import get_redoc_html, get_swagger_ui_html\\nfrom fastapi.staticfiles import StaticFiles\\nfrom fastapi import FastAPI\\nfrom fastapi.responses import HTMLResponse\\n\\napp = FastAPI(\\n    title=\"Custom Docs UI\",\\n)\\n\\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\\n\\n@app.get(\"/docs\", include_in_schema=False)\\nasync def custom_swagger_ui_html():\\n    return get_swagger_ui_html(\\n        openapi_url=app.openapi_url,\\n        title=app.title + \" - Swagger UI\",\\n        oauth2_redirect_url=app.swagger_ui_oauth2_redirect_url,\\n        swagger_js_url=\"https://unpkg.com/swagger-ui-dist@5/swagger-ui-bundle.js\",\\n        swagger_css_url=\"https://unpkg.com/swagger-ui-dist@5/swagger-ui.css\",\\n    )\\n\\n\\n@app.get(\"/redoc\", include_in_schema=False)\\nasync def custom_redoc_html():\\n    return get_redoc_html(\\n        openapi_url=app.openapi_url,\\n        title=app.title + \" - ReDoc\",\\n        redoc_js_url=\"https://unpkg.com/redoc@next/bundles/redoc.standalone.js\",\\n    )\\n```\\n\\n----------------------------------------\\n\\nTITLE: Creating a FastAPI Instance\\nDESCRIPTION: This code snippet shows how to create an instance of the FastAPI class, which serves as the main entry point for defining API endpoints.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/de/docs/tutorial/first-steps.md#_snippet_2\\n\\nLANGUAGE: Python\\nCODE:\\n```\\napp = FastAPI()\\n```\\n\\n----------------------------------------\\n\\nTITLE: Initializing FastAPI App\\nDESCRIPTION: Create a basic FastAPI application with two endpoints: `/` which returns a simple greeting, and `/items/{item_id}` which returns the item ID and an optional query parameter.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/vi/docs/index.md#_snippet_2\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom typing import Union\\n\\nfrom fastapi import FastAPI\\n\\napp = FastAPI()\\n\\n\\n@app.get(\"/\")\\ndef read_root():\\n    return {\"Hello\": \"World\"}\\n\\n\\n@app.get(\"/items/{item_id}\")\\ndef read_item(item_id: int, q: Union[str, None] = None):\\n    return {\"item_id\": item_id, \"q\": q}\\n```\\n\\n----------------------------------------\\n\\nTITLE: Creating a FastAPI Instance\\nDESCRIPTION: Creates an instance of the FastAPI class, assigning it to the variable app. This instance is the main entry point for defining API endpoints and handling requests.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/ko/docs/tutorial/first-steps.md#_snippet_2\\n\\nLANGUAGE: Python\\nCODE:\\n```\\napp = FastAPI()\\n```\\n\\n----------------------------------------\\n\\nTITLE: Initializing WebSocket endpoint\\nDESCRIPTION: Creates a WebSocket endpoint in a FastAPI application to handle incoming and outgoing messages. It defines the basic structure for handling WebSocket connections, receiving messages, and sending responses.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/ru/docs/advanced/websockets.md#_snippet_0\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom fastapi import FastAPI, WebSocket\\n\\napp = FastAPI()\\n\\n\\n@app.websocket(\"/ws\")\\nasync def websocket_endpoint(websocket: WebSocket):\\n    await websocket.accept()\\n    while True:\\n        data = await websocket.receive_text()\\n        await websocket.send_text(f\"Message text was: {data}\")\\n```\\n\\n----------------------------------------\\n\\nTITLE: Creating a FastAPI Instance with a Custom Name\\nDESCRIPTION: This code snippet shows how to create an instance of the FastAPI class and assign it to a variable named `my_awesome_api`. This instance is used to define the API endpoints.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/ja/docs/tutorial/first-steps.md#_snippet_3\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom fastapi import FastAPI\\n\\nmy_awesome_api = FastAPI()\\n\\n@my_awesome_api.get(\"/\")\\nasync def root():\\n    return {\"message\": \"Hello World\"}\\n```\\n\\n----------------------------------------\\n\\nTITLE: Creating a FastAPI Instance\\nDESCRIPTION: This code snippet shows how to create an instance of the FastAPI class. This instance, typically named \\'app\\', serves as the main entry point for defining all API endpoints and functionality.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/id/docs/tutorial/first-steps.md#_snippet_3\\n\\nLANGUAGE: Python\\nCODE:\\n```\\napp\\n```\\n\\n----------------------------------------\\n\\nTITLE: Simple FastAPI Application\\nDESCRIPTION: This code defines a basic FastAPI application with a single endpoint that returns a JSON response. It imports the FastAPI class, creates an instance of it, and defines a path operation for the root path (\\'/\\').\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/ja/docs/tutorial/first-steps.md#_snippet_0\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom fastapi import FastAPI\\n\\napp = FastAPI()\\n\\n@app.get(\"/\")\\nasync def root():\\n    return {\"message\": \"Hello World\"}\\n```\\n\\n----------------------------------------\\n\\nTITLE: Create a Basic FastAPI WebSocket Endpoint\\nDESCRIPTION: Demonstrates how to define a WebSocket endpoint in FastAPI, accept incoming connections, and echo received text messages back to the client. This forms the foundation for real-time communication.\\nSOURCE: https://github.com/tiangolo/fastapi/blob/master/docs/em/docs/advanced/websockets.md#_snippet_1\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom fastapi import FastAPI, WebSocket\\n\\napp = FastAPI()\\n\\n@app.websocket(\"/ws\")\\nasync def websocket_endpoint(websocket: WebSocket):\\n    await websocket.accept()\\n    while True:\\n        data = await websocket.receive_text()\\n        await websocket.send_text(f\"Message text was: {data}\")\\n```', name='scrap_docs', tool_call_id='call_KEcZxruChbSuTeqZ2KbZUrfe'),\n",
       " ToolMessage(content='TITLE: Validate List of Users with Pydantic TypeAdapter and httpx\\nDESCRIPTION: This example illustrates how to validate a list of user objects returned from an API using Pydantic\\'s `TypeAdapter`. It queries the `/users/` endpoint of the JSONPlaceholder API and uses `TypeAdapter(list[User])` to efficiently validate the entire list of dictionaries into a list of Pydantic `User` instances.\\nSOURCE: https://docs.pydantic.dev/latest/examples/requests\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom pprint import pprint\\n\\nimport httpx\\n\\nfrom pydantic import BaseModel, EmailStr, TypeAdapter\\n\\nclass User(BaseModel):\\n    id: int\\n    name: str\\n    email: EmailStr\\n\\nurl = \\'https://jsonplaceholder.typicode.com/users/\\'  # (1)!\\n\\nresponse = httpx.get(url)\\nresponse.raise_for_status()\\n\\nusers_list_adapter = TypeAdapter(list[User])\\n\\nusers = users_list_adapter.validate_python(response.json())\\npprint([u.name for u in users])\\n\"\"\"\\n[\\'Leanne Graham\\',\\n \\'Ervin Howell\\',\\n \\'Clementine Bauch\\',\\n \\'Patricia Lebsack\\',\\n \\'Chelsey Dietrich\\',\\n \\'Mrs. Dennis Schulist\\',\\n \\'Kurtis Weissnat\\',\\n \\'Nicholas Runolfsdottir V\\',\\n \\'Glenna Reichert\\',\\n \\'Clementina DuBuque\\']\\n\"\"\"\\n```\\n\\n----------------------------------------\\n\\nTITLE: Pydantic BaseModel.model_validate_json() API Reference\\nDESCRIPTION: API documentation for the `model_validate_json()` class method on Pydantic `BaseModel`. This method validates data provided as a JSON string or `bytes` object. It is generally considered faster for JSON payloads compared to manually parsing the data as a dictionary.\\nSOURCE: https://docs.pydantic.dev/latest/concepts/models\\n\\nLANGUAGE: APIDOC\\nCODE:\\n```\\nmodel_validate_json():\\n  Description: This validates the provided data as a JSON string or bytes object. If your incoming data is a JSON payload, this is generally considered faster (instead of manually parsing the data as a dictionary).\\n```\\n\\n----------------------------------------\\n\\nTITLE: Validate Single User Data with Pydantic and httpx\\nDESCRIPTION: This Python example demonstrates how to fetch a single user\\'s data from the JSONPlaceholder API using `httpx` and then validate it against a Pydantic `User` BaseModel. It shows the basic setup for defining a Pydantic model and using `model_validate` to parse the JSON response.\\nSOURCE: https://docs.pydantic.dev/latest/examples/requests\\n\\nLANGUAGE: python\\nCODE:\\n```\\nimport httpx\\n\\nfrom pydantic import BaseModel, EmailStr\\n\\nclass User(BaseModel):\\n    id: int\\n    name: str\\n    email: EmailStr\\n\\nurl = \\'https://jsonplaceholder.typicode.com/users/1\\'\\n\\nresponse = httpx.get(url)\\nresponse.raise_for_status()\\n\\nuser = User.model_validate(response.json())\\nprint(repr(user))\\n#> User(id=1, name=\\'Leanne Graham\\', email=\\'[email\\xa0protected]\\')\\n```\\n\\n----------------------------------------\\n\\nTITLE: Pydantic WrapValidator Custom Timestamp Validation Example\\nDESCRIPTION: Illustrates how to implement a custom timestamp validator using `WrapValidator`. This example demonstrates handling specific input values like \\'now\\' and providing a default value for invalid inputs, showcasing integration with Pydantic models.\\nSOURCE: https://docs.pydantic.dev/latest/api/functional_validators\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom datetime import datetime\\nfrom typing import Annotated\\n\\nfrom pydantic import BaseModel, ValidationError, WrapValidator\\n\\ndef validate_timestamp(v, handler):\\n    if v == \\'now\\':\\n        # we don\\'t want to bother with further validation, just return the new value\\n        return datetime.now()\\n    try:\\n        return handler(v)\\n    except ValidationError:\\n        # validation failed, in this case we want to return a default value\\n        return datetime(2000, 1, 1)\\n\\nMyTimestamp = Annotated[datetime, WrapValidator(validate_timestamp)]\\n\\nclass Model(BaseModel):\\n    a: MyTimestamp\\n\\nprint(Model(a=\\'now\\').a)\\n#> 2032-01-02 03:04:05.000006\\nprint(Model(a=\\'invalid\\').a)\\n#> 2000-01-01 00:00:00\\n```\\n\\n----------------------------------------\\n\\nTITLE: API Reference: SchemaValidator.validate_strings Method\\nDESCRIPTION: Detailed API documentation for the `validate_strings` method of `pydantic_core.SchemaValidator`, including its signature, parameters, return values, and potential exceptions. This method validates string inputs that are not necessarily JSON.\\nSOURCE: https://docs.pydantic.dev/latest/api/pydantic_core\\n\\nLANGUAGE: APIDOC\\nCODE:\\n```\\nMethod: validate_strings\\nSignature: validate_strings(\\n    input: _StringInput,\\n    *,\\n    strict: bool | None = None,\\n    context: Any | None = None,\\n    allow_partial: (\\n        bool | Literal[\"off\", \"on\", \"trailing-strings\"]\\n    ) = False,\\n    by_alias: bool | None = None,\\n    by_name: bool | None = None\\n) -> Any\\nDescription: Validate a string against the schema and return the validated Python object. This is similar to `validate_json` but applies to scenarios where the input will be a string but not JSON data, e.g. URL fragments, query parameters, etc.\\nParameters:\\n  - name: input\\n    type: _StringInput\\n    description: The input as a string, or bytes/bytearray if `strict=False`.\\n    default: *required*\\n  - name: strict\\n    type: bool | None\\n    description: Whether to validate the object in strict mode. If `None`, the value of `CoreConfig.strict` is used.\\n    default: None\\n  - name: context\\n    type: Any | None\\n    description: The context to use for validation, this is passed to functional validators as `info.context`.\\n    default: None\\n  - name: allow_partial\\n    type: bool | Literal[\\'off\\', \\'on\\', \\'trailing-strings\\']\\n    description: Whether to allow partial validation; if `True` errors in the last element of sequences and mappings are ignored. \\'trailing-strings\\' means any final unfinished JSON string is included in the result.\\n    default: False\\n  - name: by_alias\\n    type: bool | None\\n    description: Whether to use the field\\'s alias when validating against the provided input data.\\n    default: None\\n  - name: by_name\\n    type: bool | None\\n    description: Whether to use the field\\'s name when validating against the provided input data.\\n    default: None\\nRaises:\\n  - type: ValidationError\\n    description: If validation fails or if the JSON data is invalid.\\n  - type: Exception\\n    description: Other error types maybe raised if internal errors occur.\\nReturns:\\n  - type: Any\\n    description: The validated Python object.\\n```\\n\\n----------------------------------------\\n\\nTITLE: Validate JSON Data with Pydantic Model\\nDESCRIPTION: This class method validates JSON data (string, bytes, or bytearray) against the Pydantic model. It supports strict type enforcement and allows for passing extra context. Validation can be configured to use field aliases or names.\\nSOURCE: https://docs.pydantic.dev/latest/api/base_model\\n\\nLANGUAGE: APIDOC\\nCODE:\\n```\\nclassmethod model_validate_json(\\n  json_data: str | bytes | bytearray,\\n  *,\\n  strict: bool | None = None,\\n  context: Any | None = None,\\n  by_alias: bool | None = None,\\n  by_name: bool | None = None\\n) -> Self\\n\\nParameters:\\n  json_data: The JSON data to validate.\\n  strict: Whether to enforce types strictly.\\n  context: Extra variables to pass to the validator.\\n  by_alias: Whether to use the field\\'s alias when validating against the provided input data.\\n  by_name: Whether to use the field\\'s name when validating against the provided input data.\\n\\nReturns:\\n  The validated Pydantic model.\\n\\nRaises:\\n  ValidationError: If `json_data` is not a JSON string or the object could not be validated.\\n```\\n\\n----------------------------------------\\n\\nTITLE: Pydantic Model Wrap Validator for Flexible Validation Control\\nDESCRIPTION: This example showcases a \\'wrap\\' model validator in Pydantic, providing the most flexibility. It allows code to run before or after Pydantic\\'s internal validation process. The snippet demonstrates logging a `ValidationError` if validation fails, enabling custom error handling or early termination of validation.\\nSOURCE: https://docs.pydantic.dev/latest/concepts/validators\\n\\nLANGUAGE: python\\nCODE:\\n```\\nimport logging\\nfrom typing import Any\\n\\nfrom typing_extensions import Self\\n\\nfrom pydantic import BaseModel, ModelWrapValidatorHandler, ValidationError, model_validator\\n\\nclass UserModel(BaseModel):\\n    username: str\\n\\n    @model_validator(mode=\\'wrap\\')\\n    @classmethod\\n    def log_failed_validation(cls, data: Any, handler: ModelWrapValidatorHandler[Self]) -> Self:\\n        try:\\n            return handler(data)\\n        except ValidationError:\\n            logging.error(\\'Model %s failed to validate with data %s\\', cls, data)\\n            raise\\n```\\n\\n----------------------------------------\\n\\nTITLE: Map Legacy Pydantic Validators to Pipeline API Equivalents\\nDESCRIPTION: This snippet illustrates how the experimental pipeline API\\'s `validate_as` and `transform` methods offer a more type-safe and composable alternative to Pydantic\\'s traditional `BeforeValidator`, `AfterValidator`, and `WrapValidator`. It provides direct examples for stripping whitespace before parsing, multiplying a value after parsing, and combining both pre- and post-processing steps within a single `Annotated` type.\\nSOURCE: https://docs.pydantic.dev/latest/concepts/experimental\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom typing import Annotated\\n\\nfrom pydantic.experimental.pipeline import transform, validate_as\\n\\n# BeforeValidator\\nAnnotated[int, validate_as(str).str_strip().validate_as(...)]  # (1)!\\n# AfterValidator\\nAnnotated[int, transform(lambda x: x * 2)]  # (2)!\\n# WrapValidator\\nAnnotated[\\n    int,\\n    validate_as(str)\\n    .str_strip()\\n    .validate_as(...)\\n    .transform(lambda x: x * 2),  # (3)!\\n]\\n```\\n\\n----------------------------------------\\n\\nTITLE: Pydantic Validation and Serialization for IntEnum\\nDESCRIPTION: Describes Pydantic\\'s handling of `enum.IntEnum` types. It validates values as valid `IntEnum` instances and checks for valid members when dealing with subclasses.\\nSOURCE: https://docs.pydantic.dev/latest/api/standard_library_types\\n\\nLANGUAGE: APIDOC\\nCODE:\\n```\\nType: enum.IntEnum\\nValidation:\\n  - Validates value as a valid IntEnum instance.\\n  - For subclasses: Validates value as a valid member of the integer enum.\\n```\\n\\n----------------------------------------\\n\\nTITLE: Validate Partial JSON with TypeAdapter and Error Ignoring\\nDESCRIPTION: This Python snippet demonstrates how to use Pydantic\\'s `TypeAdapter` with `experimental_allow_partial=True` to parse an incomplete JSON string. It shows that errors in the last element of the input (e.g., a truncated string) are ignored, allowing the preceding valid data to be successfully validated and returned.\\nSOURCE: https://docs.pydantic.dev/latest/concepts/experimental\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom typing import Annotated\\n\\nfrom annotated_types import MinLen\\n\\nfrom pydantic import BaseModel, TypeAdapter\\n\\nclass MyModel(BaseModel):\\n    a: int\\n    b: Annotated[str, MinLen(5)]\\n\\nta = TypeAdapter(list[MyModel])\\nv = ta.validate_json(\\n    \\'[{\"a\": 1, \"b\": \"12345\"}, {\"a\": 1,\\',\\n    experimental_allow_partial=True,\\n)\\nprint(v)\\n#> [MyModel(a=1, b=\\'12345\\')]\\n```\\n\\n----------------------------------------\\n\\nTITLE: Pydantic Model After Validator for Post-Initialization Checks\\nDESCRIPTION: This example demonstrates an \\'after\\' model validator in Pydantic. It runs after the entire model has been validated, acting as a post-initialization hook. The validator checks if the `password` and `password_repeat` fields match, raising a `ValueError` if they don\\'t, and must return the validated `Self` instance.\\nSOURCE: https://docs.pydantic.dev/latest/concepts/validators\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom typing_extensions import Self\\n\\nfrom pydantic import BaseModel, model_validator\\n\\nclass UserModel(BaseModel):\\n    username: str\\n    password: str\\n    password_repeat: str\\n\\n    @model_validator(mode=\\'after\\')\\n    def check_passwords_match(self) -> Self:\\n        if self.password != self.password_repeat:\\n            raise ValueError(\\'Passwords do not match\\')\\n        return self\\n```\\n\\n----------------------------------------\\n\\nTITLE: Validate Even Number with Pydantic field_validator (Decorator)\\nDESCRIPTION: Illustrates defining an \\'after\\' validator using the `@field_validator` decorator in Pydantic to check if an integer field is even. It shows how to apply the validator to a class method and handle validation errors.\\nSOURCE: https://docs.pydantic.dev/latest/concepts/validators\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom pydantic import BaseModel, ValidationError, field_validator\\n\\nclass Model(BaseModel):\\n    number: int\\n\\n    @field_validator(\\'number\\', mode=\\'after\\')  # (1)!\\n    @classmethod\\n    def is_even(cls, value: int) -> int:\\n        if value % 2 == 1:\\n            raise ValueError(f\\'{value} is not an even number\\')\\n        return value  # (2)!\\n\\ntry:\\n    Model(number=1)\\nexcept ValidationError as err:\\n    print(err)\\n    \"\"\"\\n    1 validation error for Model\\n    number\\n      Value error, 1 is not an even number [type=value_error, input_value=1, input_type=int]\\n    \"\"\"\\n```\\n\\n----------------------------------------\\n\\nTITLE: Pydantic Wrap Validators: Flexible Control with Handler\\nDESCRIPTION: Explores `wrap` validators, the most flexible type, allowing custom logic to run before or after Pydantic\\'s internal validation handler. This enables advanced use cases like custom error handling, modifying input based on validation errors, or conditionally skipping Pydantic\\'s validation. The `handler` parameter is crucial for delegating to Pydantic\\'s default validation.\\nSOURCE: https://docs.pydantic.dev/latest/concepts/validators\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom typing import Any\\n\\nfrom typing import Annotated\\n\\nfrom pydantic import BaseModel, Field, ValidationError, ValidatorFunctionWrapHandler, WrapValidator\\n\\ndef truncate(value: Any, handler: ValidatorFunctionWrapHandler) -> str:\\n    try:\\n        return handler(value)\\n    except ValidationError as err:\\n        if err.errors()[0][\\'type\\'] == \\'string_too_long\\':\\n            return handler(value[:5])\\n        else:\\n            raise\\n\\nclass Model(BaseModel):\\n```\\n\\n----------------------------------------\\n\\nTITLE: Pydantic Partial Validation of Incomplete JSON\\nDESCRIPTION: This snippet demonstrates how Pydantic\\'s `TypeAdapter` with `experimental_allow_partial=True` handles incomplete JSON input. It shows that even with a truncated JSON string, the validation passes, and the partially valid data is returned, omitting the field that failed validation due to incompleteness. It also shows that a complete but invalid JSON (due to `MinLen` constraint) behaves similarly if the error is in the last field.\\nSOURCE: https://docs.pydantic.dev/latest/concepts/experimental\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom typing import Annotated\\n\\nfrom annotated_types import MinLen\\nfrom typing_extensions import TypedDict\\n\\nfrom pydantic import TypeAdapter\\n\\nclass Foobar(TypedDict, total=False):\\n    a: int\\n    b: Annotated[str, MinLen(5)]\\n\\nta = TypeAdapter(Foobar)\\n\\nv = ta.validate_json(\\n    \\'{\"a\": 1, \"b\": \"12\\', experimental_allow_partial=True  # (1)!\\n)\\nprint(v)\\n#> {\\'a\\': 1}\\n\\nv = ta.validate_json(\\n    \\'{\"a\": 1, \"b\": \"12\"}\\', experimental_allow_partial=True  # (2)!\\n)\\nprint(v)\\n#> {\\'a\\': 1}\\n```\\n\\n----------------------------------------\\n\\nTITLE: Pydantic Validation for Enum Types\\nDESCRIPTION: Describes Pydantic\\'s integration with Python\\'s standard `enum` classes. It validates values as valid `Enum` instances and checks for valid members when dealing with subclasses of `enum.Enum`.\\nSOURCE: https://docs.pydantic.dev/latest/api/standard_library_types\\n\\nLANGUAGE: APIDOC\\nCODE:\\n```\\nType: enum.Enum\\nValidation:\\n  - Validates value as a valid Enum instance.\\n  - For subclasses: Validates value as a valid member of the enum.\\n```\\n\\n----------------------------------------\\n\\nTITLE: Demonstrate Pydantic Partial Validation with TypeAdapter\\nDESCRIPTION: This comprehensive snippet demonstrates the \\'experimental_allow_partial\\' flag using \\'TypeAdapter.validate_json()\\' and \\'TypeAdapter.validate_python()\\'. It defines a \\'Foobar\\' TypedDict with required and optional fields, then shows various partial JSON strings and Python objects being validated. Examples cover cases where required fields are missing (leading to item omission), optional fields fail validation (leading to omission), and the effect of \\'trailing-strings\\' mode on incomplete string values.\\nSOURCE: https://docs.pydantic.dev/latest/concepts/experimental\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom typing import Annotated\\n\\nfrom annotated_types import MinLen\\nfrom typing_extensions import NotRequired, TypedDict\\n\\nfrom pydantic import TypeAdapter\\n\\nclass Foobar(TypedDict):\\n    a: int\\n    b: NotRequired[float]\\n    c: NotRequired[Annotated[str, MinLen(5)]]\\n\\nta = TypeAdapter(list[Foobar])\\n\\nv = ta.validate_json(\\'[{\"a\": 1, \"b\"\\', experimental_allow_partial=True)\\nprint(v)\\n#>[{\\'a\\': 1}]\\n\\nv = ta.validate_json(\\n    \\'[{\"a\": 1, \"b\": 1.0, \"c\": \"abcd\\', experimental_allow_partial=True\\n)\\nprint(v)\\n#>[{\\'a\\': 1, \\'b\\': 1.0}]\\n\\nv = ta.validate_json(\\n    \\'[{\"b\": 1.0, \"c\": \"abcde\"\\', experimental_allow_partial=True\\n)\\nprint(v)\\n#>[]\\n\\nv = ta.validate_json(\\n    \\'[{\"a\": 1, \"b\": 1.0, \"c\": \"abcde\"},{\"a\": \\',\\n    experimental_allow_partial=True\\n)\\nprint(v)\\n#>[{\\'a\\': 1, \\'b\\': 1.0, \\'c\\': \\'abcde\\'}]\\n\\nv = ta.validate_python([{\\'a\\': 1}], experimental_allow_partial=True)\\nprint(v)\\n#>[{\\'a\\': 1}]\\n\\nv = ta.validate_python(\\n    [{\\'a\\': 1, \\'b\\': 1.0, \\'c\\': \\'abcd\\'}], experimental_allow_partial=True\\n)\\nprint(v)\\n#>[{\\'a\\': 1, \\'b\\': 1.0}]\\n\\nv = ta.validate_json(\\n    \\'[{\"a\": 1, \"b\": 1.0, \"c\": \"abcdefg\\',\\n    experimental_allow_partial=\\'trailing-strings\\'\\n)\\nprint(v)\\n#>[{\\'a\\': 1, \\'b\\': 1.0, \\'c\\': \\'abcdefg\\'}]\\n```\\n\\n----------------------------------------\\n\\nTITLE: Pydantic Functional Validators API Reference\\nDESCRIPTION: API references for various Pydantic functional validators and the `field_validator` decorator, used for defining custom validation logic.\\nSOURCE: https://docs.pydantic.dev/latest/concepts/validators\\n\\nLANGUAGE: APIDOC\\nCODE:\\n```\\npydantic.functional_validators.WrapValidator\\npydantic.functional_validators.PlainValidator\\npydantic.functional_validators.BeforeValidator\\npydantic.functional_validators.AfterValidator\\npydantic.functional_validators.field_validator\\n```\\n\\n----------------------------------------\\n\\nTITLE: Validate HTTP/HTTPS URLs with Pydantic HttpUrl\\nDESCRIPTION: Demonstrates how to use `pydantic.HttpUrl` within a `BaseModel` to validate HTTP and HTTPS URLs. It includes examples of successful validation and handling `ValidationError` for invalid schemes (e.g., \\'ftp\\') or malformed URLs.\\nSOURCE: https://docs.pydantic.dev/latest/api/networks\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom pydantic import BaseModel, HttpUrl, ValidationError\\n\\nclass MyModel(BaseModel):\\n    url: HttpUrl\\n\\nm = MyModel(url=\\'http://www.example.com\\')\\nprint(m.url)\\n# http://www.example.com/\\n\\ntry:\\n    MyModel(url=\\'ftp://invalid.url\\')\\nexcept ValidationError as e:\\n    print(e)\\n    \\'\\'\\'\\n    1 validation error for MyModel\\n    url\\n      URL scheme should be \\'http\\' or \\'https\\' [type=url_scheme, input_value=\\'ftp://invalid.url\\', input_type=str]\\n    \\'\\'\\'\\n\\ntry:\\n    MyModel(url=\\'not a url\\')\\nexcept ValidationError as e:\\n    print(e)\\n    \\'\\'\\'\\n    1 validation error for MyModel\\n    url\\n      Input should be a valid URL, relative URL without a base [type=url_parsing, input_value=\\'not a url\\', input_type=str]\\n    \\'\\'\\'\\n```\\n\\n----------------------------------------\\n\\nTITLE: Validate Pydantic Model Field Assignment with SchemaValidator\\nDESCRIPTION: This method validates an assignment to a specific field on a Pydantic model instance. It allows for strict validation, attribute-based validation, and context passing, returning the validated model data or a detailed tuple.\\nSOURCE: https://docs.pydantic.dev/latest/api/pydantic_core\\n\\nLANGUAGE: python\\nCODE:\\n```\\nvalidate_assignment(\\n    obj: Any,\\n    field_name: str,\\n    field_value: Any,\\n    *,\\n    strict: bool | None = None,\\n    from_attributes: bool | None = None,\\n    context: Any | None = None,\\n    by_alias: bool | None = None,\\n    by_name: bool | None = None\\n) -> (\\n    dict[str, Any]\\n    | tuple[dict[str, Any], dict[str, Any] | None, set[str]]\\n)\\n```\\n\\nLANGUAGE: APIDOC\\nCODE:\\n```\\nMethod: validate_assignment\\nDescription: Validate an assignment to a field on a model.\\nParameters:\\n  obj:\\n    Type: Any\\n    Description: The model instance being assigned to.\\n    Default: required\\n  field_name:\\n    Type: str\\n    Description: The name of the field to validate assignment for.\\n    Default: required\\n  field_value:\\n    Type: Any\\n    Description: The value to assign to the field.\\n    Default: required\\n  strict:\\n    Type: bool | None\\n    Description: Whether to validate the object in strict mode. If None, the value of CoreConfig.strict is used.\\n    Default: None\\n  from_attributes:\\n    Type: bool | None\\n    Description: Whether to validate objects as inputs to models by extracting attributes. If None, the value of CoreConfig.from_attributes is used.\\n    Default: None\\n  context:\\n    Type: Any | None\\n    Description: The context to use for validation, this is passed to functional validators as info.context.\\n    Default: None\\n  by_alias:\\n    Type: bool | None\\n    Description: Whether to use the field\\'s alias when validating against the provided input data.\\n    Default: None\\n  by_name:\\n    Type: bool | None\\n    Description: Whether to use the field\\'s name when validating against the provided input data.\\n    Default: None\\nRaises:\\n  ValidationError: If validation fails.\\n  Exception: Other error types maybe raised if internal errors occur.\\nReturns:\\n  Type: dict[str, Any] | tuple[dict[str, Any], dict[str, Any] | None, set[str]]\\n  Description: Either the model dict or a tuple of (model_data, model_extra, fields_set)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Pydantic Strict JSON Parsing with model_validate_json\\nDESCRIPTION: Demonstrates how Pydantic\\'s `model_validate_json` method handles JSON strings with `strict=True` enabled, allowing type coercion for `date` and `tuple` from string/array inputs. It contrasts this with `model_validate`, which raises validation errors for non-native types when strict mode is active.\\nSOURCE: https://docs.pydantic.dev/latest/concepts/json\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom datetime import date\\n\\nfrom pydantic import BaseModel, ConfigDict, ValidationError\\n\\nclass Event(BaseModel):\\n    model_config = ConfigDict(strict=True)\\n\\n    when: date\\n    where: tuple[int, int]\\n\\njson_data = \\'{\"when\": \"1987-01-28\", \"where\": [51, -1]}\\'\\nprint(Event.model_validate_json(json_data))  # (1)!\\n#> when=datetime.date(1987, 1, 28) where=(51, -1)\\n\\ntry:\\n    Event.model_validate({\\'when\\': \\'1987-01-28\\', \\'where\\': [51, -1]})  # (2)!\\nexcept ValidationError as e:\\n    print(e)\\n    \"\"\"\\n    2 validation errors for Event\\n    when\\n      Input should be a valid date [type=date_type, input_value=\\'1987-01-28\\', input_type=str]\\n    where\\n      Input should be a valid tuple [type=tuple_type, input_value=[51, -1], input_type=list]\\n    \"\"\"\\n```', name='scrap_docs', tool_call_id='call_IvOZHecpORfY9Z5kINTPfRaw'),\n",
       " ToolMessage(content='TITLE: Chain ChatContextual Model with StrOutputParser\\nDESCRIPTION: Illustrates how to chain the `ChatContextual` model with `StrOutputParser` to process and format the model\\'s output. This example shows a simple chaining setup.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/chat/contextual\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nchain = llm | StrOutputParser\\n\\nchain.invoke(\\n    messages, knowledge=knowledge, systemp_prompt=system_prompt, avoid_commentary=True\\n)\\n\\n```\\n\\n----------------------------------------\\n\\nTITLE: Batch Process Messages with Python Generative Model\\nDESCRIPTION: This Python snippet demonstrates how to send a list of messages to a generative AI model for efficient batch processing. It\\'s useful for handling multiple inputs simultaneously, reducing overhead compared to individual calls.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/llms/google_vertex_ai_palm\\n\\nLANGUAGE: python\\nCODE:\\n```\\nmodel.batch([message])\\n```\\n\\n----------------------------------------\\n\\nTITLE: Direct Usage of Model2Vec StaticModel\\nDESCRIPTION: Illustrates how to directly use the `StaticModel` from the `model2vec` library to load models, encode sentences into embeddings, and generate sequences of token embeddings.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/text_embedding/model2vec\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom model2vec import StaticModel\\n\\n# Load a model from the HuggingFace hub (in this case the potion-base-8M model)\\nmodel = StaticModel.from_pretrained(\"minishlab/potion-base-8M\")\\n\\n# Make embeddings\\nembeddings = model.encode([\"It\\'s dangerous to go alone!\", \"It\\'s a secret to everybody.\"])\\n\\n# Make sequences of token embeddings\\ntoken_embeddings = model.encode_as_sequence([\"It\\'s dangerous to go alone!\", \"It\\'s a secret to everybody.\"])\\n```\\n\\n----------------------------------------\\n\\nTITLE: Integrate Processed Messages with ChatOpenAI Model\\nDESCRIPTION: Shows an example of using the processed `ChatSession` messages as input for a `ChatOpenAI` model. The code iterates through the streamed responses from the language model, printing each chunk of content as it becomes available.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/chat_loaders/telegram\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom langchain_openai import ChatOpenAI\\n\\nllm = ChatOpenAI()\\n\\nfor chunk in llm.stream(messages[0][\"messages\"]):\\n    print(chunk.content, end=\"\", flush=True)\\n```\\n\\n----------------------------------------\\n\\nTITLE: RecursiveCharacterTextSplitter API Reference\\nDESCRIPTION: Provides a reference to the `RecursiveCharacterTextSplitter` class from `langchain_text_splitters`. This utility is designed to split large text documents into smaller, manageable chunks, which is often a prerequisite for processing by language models or for indexing in vector databases.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/document_loaders/image_captions\\n\\nLANGUAGE: APIDOC\\nCODE:\\n```\\nRecursiveCharacterTextSplitter\\n```\\n\\n----------------------------------------\\n\\nTITLE: Perform Batch CPU Inference with Quantized Hugging Face Model\\nDESCRIPTION: This example demonstrates how to run inference in batch mode on the CPU. It uses the `chain.batch()` method to process multiple questions efficiently, binding a stop sequence to the LLM.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/llms/weight_only_quantization\\n\\nLANGUAGE: python\\nCODE:\\n```\\nconf = WeightOnlyQuantConfig(weight_dtype=\"nf4\")\\nllm = WeightOnlyQuantPipeline.from_model_id(\\n    model_id=\"google/flan-t5-large\",\\n    task=\"text2text-generation\",\\n    quantization_config=conf,\\n    pipeline_kwargs={\"max_new_tokens\": 10},\\n)\\n\\nchain = prompt | llm.bind(stop=[\"\\\\\\\\n\\\\\\\\n\"])\\n\\nquestions = []\\nfor i in range(4):\\n    questions.append({\"question\": f\"What is the number {i} in french?\"})\\n\\nanswers = chain.batch(questions)\\nfor answer in answers:\\n    print(answer)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Define Custom Model Loading and Inference Functions for Embeddings\\nDESCRIPTION: This snippet defines `get_pipeline` to load a custom Hugging Face model and create a feature extraction pipeline, and `inference_fn` for processing prompts through this pipeline. These functions are crucial for `SelfHostedEmbeddings` when a custom model and inference logic are required, allowing flexible model integration.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/text_embedding/self-hosted\\n\\nLANGUAGE: python\\nCODE:\\n```\\ndef get_pipeline():\\n    from transformers import (\\n        AutoModelForCausalLM,\\n        AutoTokenizer,\\n        pipeline,\\n    )\\n\\n    model_id = \"facebook/bart-base\"\\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\\n    model = AutoModelForCausalLM.from_pretrained(model_id)\\n    return pipeline(\"feature-extraction\", model=model, tokenizer=tokenizer)\\n\\ndef inference_fn(pipeline, prompt):\\n    # Return last hidden state of the model\\n    if isinstance(prompt, list):\\n        return [emb[0][-1] for emb in pipeline(prompt)]\\n    return pipeline(prompt)[0][-1]\\n```\\n\\n----------------------------------------\\n\\nTITLE: Embed Multiple Documents using LangChain\\'s OpenAIEmbeddings\\nDESCRIPTION: This Python snippet demonstrates how to use LangChain\\'s `OpenAIEmbeddings` model to generate embeddings for a list of multiple text documents. It utilizes the `embed_documents` method, which is optimized for batch processing of texts. The output shows the number of embeddings generated and the dimensionality of each embedding.\\nSOURCE: https://python.langchain.com/docs/introduction/concepts/embedding_models\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom langchain_openai import OpenAIEmbeddings\\nembeddings_model = OpenAIEmbeddings()\\nembeddings = embeddings_model.embed_documents(\\n    [\\n        \"Hi there!\",\\n        \"Oh, hello!\",\\n        \"What\\'s your name?\",\\n        \"My friends call me World\",\\n        \"Hello World!\"\\n    ]\\n)\\nlen(embeddings), len(embeddings[0])\\n(5, 1536)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Batch Invoke Cohere Model\\nDESCRIPTION: Demonstrates how to process multiple inputs in a single batch call to the Cohere model using `model.batch()`. This can be more efficient for processing several requests at once.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/llms/cohere\\n\\nLANGUAGE: python\\nCODE:\\n```\\nmodel.batch([message])\\n```\\n\\n----------------------------------------\\n\\nTITLE: Export and Load OpenVINO IR Model Locally\\nDESCRIPTION: Explains how to export an embedding model to the OpenVINO IR format using \\'ov_embeddings.save_model()\\' and then load it back from a local directory for optimized inference. This allows for persistent and efficient model usage.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/text_embedding/openvino\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom pathlib import Path\\n\\nov_model_dir = \"all-mpnet-base-v2-ov\"\\nif not Path(ov_model_dir).exists():\\n    ov_embeddings.save_model(ov_model_dir)\\n\\nov_embeddings = OpenVINOEmbeddings(\\n    model_name_or_path=ov_model_dir,\\n    model_kwargs=model_kwargs,\\n    encode_kwargs=encode_kwargs,\\n)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Import UnstructuredRTFLoader in LangChain\\nDESCRIPTION: This loader enables the extraction of text and data from Rich Text Format (RTF) files. It utilizes the `unstructured` library to parse RTF documents, making their content available for language model processing.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/providers/unstructured\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom langchain_community.document_loaders import UnstructuredRTFLoader\\n```\\n\\n----------------------------------------\\n\\nTITLE: Invoke LLM with Tool Results for Final Answer\\nDESCRIPTION: This final step invokes the language model again, providing the complete conversation history including the original query, the model\\'s tool calls, and the results from executing those tools. The model uses this comprehensive context to generate a coherent and final answer to the initial human query.\\nSOURCE: https://python.langchain.com/docs/introduction/how_to/tool_results_pass_to_model\\n\\nLANGUAGE: python\\nCODE:\\n```\\nllm_with_tools.invoke(messages)\\n```\\n\\n----------------------------------------\\n\\nTITLE: LangChain Core Components API Reference\\nDESCRIPTION: API specifications for essential LangChain components used in building advanced language model applications, including chains for question answering, OpenAI integrations for LLMs and embeddings, and utilities for text processing.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/document_loaders/psychic\\n\\nLANGUAGE: APIDOC\\nCODE:\\n```\\nRetrievalQAWithSourcesChain:\\n  Purpose: A chain for question answering that also returns the sources of the answer.\\n  from_chain_type(llm: BaseLanguageModel, chain_type: str, retriever: BaseRetriever, **kwargs) -> RetrievalQAWithSourcesChain\\n    llm: The language model to use.\\n    chain_type: The type of chain to use (e.g., \"stuff\", \"map_reduce\").\\n    retriever: The retriever to fetch relevant documents.\\n```\\n\\nLANGUAGE: APIDOC\\nCODE:\\n```\\nOpenAI:\\n  Purpose: LangChain wrapper for OpenAI large language models (LLMs).\\n  __init__(temperature: float = 0.7, **kwargs)\\n    temperature: Controls the randomness of the output (0.0 for deterministic).\\n```\\n\\nLANGUAGE: APIDOC\\nCODE:\\n```\\nOpenAIEmbeddings:\\n  Purpose: LangChain wrapper for OpenAI embedding models.\\n  __init__(**kwargs)\\n  embed_documents(texts: List[str]) -> List[List[float]]\\n    texts: A list of text documents to embed.\\n    Returns: A list of embedding vectors.\\n```\\n\\nLANGUAGE: APIDOC\\nCODE:\\n```\\nCharacterTextSplitter:\\n  Purpose: Splits text into chunks based on a character.\\n  __init__(chunk_size: int = 4000, chunk_overlap: int = 200, separator: str = \"\\\\\\\\n\\\\\\\\n\", **kwargs)\\n    chunk_size: The maximum size of each chunk.\\n    chunk_overlap: The number of characters to overlap between chunks.\\n    separator: The character to split the text by.\\n  split_documents(documents: List[Document]) -> List[Document]\\n    documents: A list of documents to split.\\n    Returns: A list of split document chunks.\\n```\\n\\n----------------------------------------\\n\\nTITLE: Example Output: Model Generated Tool Calls\\nDESCRIPTION: This snippet shows an example of the `tool_calls` output generated by the language model in response to a query. It demonstrates how the model identifies and structures calls to the \\'multiply\\' and \\'add\\' tools with their respective arguments.\\nSOURCE: https://python.langchain.com/docs/introduction/how_to/tool_results_pass_to_model\\n\\nLANGUAGE: text\\nCODE:\\n```\\n[{\\'name\\': \\'multiply\\', \\'args\\': {\\'a\\': 3, \\'b\\': 12}, \\'id\\': \\'call_GPGPE943GORirhIAYnWv00rK\\', \\'type\\': \\'tool_call\\'}, {\\'name\\': \\'add\\', \\'args\\': {\\'a\\': 11, \\'b\\': 49}, \\'id\\': \\'call_dm8o64ZrY3WFZHAvCh1bEJ6i\\', \\'type\\': \\'tool_call\\'}]\\n```\\n\\n----------------------------------------\\n\\nTITLE: Index and Retrieve Data with ModelScope Embeddings\\nDESCRIPTION: This Python example shows how to create an `InMemoryVectorStore` using a sample text and the previously instantiated `ModelScopeEmbeddings` object. It then demonstrates how to convert the vector store into a retriever, which is a common step in retrieval-augmented generation (RAG) flows.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/text_embedding/modelscope_embedding\\n\\nLANGUAGE: python\\nCODE:\\n```\\n# Create a vector store with a sample text\\nfrom langchain_core.vectorstores import InMemoryVectorStore\\n\\ntext = \"LangChain is the framework for building context-aware reasoning applications\"\\n\\nvectorstore = InMemoryVectorStore.from_texts(\\n    [text],\\n    embedding=embeddings,\\n)\\n\\n# Use the vectorstore as a retriever\\nretriever = vectorstore.as_retriever()\\n```\\n\\n----------------------------------------\\n\\nTITLE: LangChain API References for XML Parsing\\nDESCRIPTION: This section provides references to key LangChain API components used in the XML output parsing process, including the chat model, XML parser, and prompt templating utility.\\nSOURCE: https://python.langchain.com/docs/introduction/how_to/output_parser_xml\\n\\nLANGUAGE: APIDOC\\nCODE:\\n```\\nChatAnthropic\\nXMLOutputParser\\nPromptTemplate\\n```\\n\\n----------------------------------------\\n\\nTITLE: Configure VLLM for Distributed Inference with Multiple GPUs\\nDESCRIPTION: This code shows how to enable distributed tensor-parallel inference with VLLM by setting the `tensor_parallel_size` parameter. It configures the LLM to utilize multiple GPUs for faster processing, which is crucial for large models.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/llms/vllm\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom langchain_community.llms import VLLM\\n\\nllm = VLLM(\\n    model=\"mosaicml/mpt-30b\",\\n    tensor_parallel_size=4,\\n    trust_remote_code=True,  # mandatory for hf models\\n)\\n\\nllm.invoke(\"What is the future of AI?\")\\n```\\n\\n----------------------------------------\\n\\nTITLE: Invoke Tools with Model-Generated Arguments and Append Results\\nDESCRIPTION: This code iterates through the tool calls generated by the model, selects the appropriate tool, and invokes it with the provided arguments. The result of each tool invocation is automatically wrapped in a `ToolMessage` and appended to the conversation history, preparing it for the next model invocation.\\nSOURCE: https://python.langchain.com/docs/introduction/how_to/tool_results_pass_to_model\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfor tool_call in ai_msg.tool_calls:\\n    selected_tool = {\"add\": add, \"multiply\": multiply}[tool_call[\"name\"].lower()]\\n    tool_msg = selected_tool.invoke(tool_call)\\n    messages.append(tool_msg)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Batch Process Prompts with NVIDIA LLM\\nDESCRIPTION: Shows how to use the \\'batch\\' method to send a list of prompts to the NVIDIA LLM for concurrent processing. This is efficient for handling multiple requests simultaneously.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/llms/nvidia_ai_endpoints\\n\\nLANGUAGE: python\\nCODE:\\n```\\nllm.batch([prompt])\\n```\\n\\n----------------------------------------\\n\\nTITLE: LangChain API References for Metadata Tagger Components\\nDESCRIPTION: Provides a structured overview of key LangChain components utilized in the OpenAI metadata tagging process, including the function to create the tagger, the base document class, and the OpenAI chat model.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/document_transformers/openai_metadata_tagger\\n\\nLANGUAGE: APIDOC\\nCODE:\\n```\\ncreate_metadata_tagger:\\n  Description: Function to initialize the OpenAIMetadataTagger document transformer.\\n  Parameters:\\n    metadata_schema: Union[Dict, Type[BaseModel]] - The schema for metadata extraction (JSON Schema or Pydantic Model).\\n    llm: BaseChatModel - An OpenAI chat model instance with functions support.\\n  Returns: DocumentTransformer\\n\\nDocument:\\n  Description: Base class for representing a document in LangChain.\\n  Properties:\\n    page_content: str - The main textual content of the document.\\n    metadata: Dict - A dictionary of arbitrary metadata associated with the document.\\n\\nChatOpenAI:\\n  Description: LangChain wrapper for OpenAI chat models.\\n  Parameters:\\n    model: str - The name of the OpenAI model to use (e.g., \\'gpt-3.5-turbo-0613\\').\\n    temperature: float - Controls randomness of output (0.0 to 1.0).\\n```\\n\\n----------------------------------------\\n\\nTITLE: Process Tool Call Results with LangChain and Google Generative AI\\nDESCRIPTION: This Python snippet demonstrates how to handle tool call results from a language model. It uses `ToolMessage` to pass the output of a tool (e.g., `get_weather`) back to the LLM, allowing the model to continue the conversation with the tool\\'s response. The example shows the structure of a tool call and the subsequent `AIMessage` response.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/chat/google_generative_ai\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom langchain_core.messages import ToolMessage\\n\\ntool_message = ToolMessage(\\n    content=get_weather(*ai_msg.tool_calls[0][\"args\"]),\\n    tool_call_id=ai_msg.tool_calls[0][\"id\"],\\n)\\nllm_with_tools.invoke([ai_msg, tool_message])\\n```\\n\\nLANGUAGE: APIDOC\\nCODE:\\n```\\nAPI Reference:\\n- tool\\n- ChatGoogleGenerativeAI\\n- ToolMessage\\n```\\n\\n----------------------------------------\\n\\nTITLE: Embed Multiple Texts with LangChain\\nDESCRIPTION: This Python snippet demonstrates how to embed multiple text documents into numerical vectors using an embedding model. It utilizes the `embed_documents` method to process a list of texts and then prints the initial portion of each resulting vector.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/text_embedding/fireworks\\n\\nLANGUAGE: python\\nCODE:\\n```\\ntext2 = (\\n    \"LangGraph is a library for building stateful, multi-actor applications with LLMs\"\\n)\\ntwo_vectors = embeddings.embed_documents([text, text2])\\nfor vector in two_vectors:\\n    print(str(vector)[:100])  # Show the first 100 characters of the vector\\n```\\n\\n----------------------------------------\\n\\nTITLE: Python Example: Invoking a LangChain Chain with Custom Parser\\nDESCRIPTION: Shows how to construct and invoke a LangChain chain by piping a prompt, an LLM (language model), and the custom `extract_json` parser. This demonstrates a common pattern for processing LLM outputs to extract structured data.\\nSOURCE: https://python.langchain.com/docs/introduction/how_to/structured_output\\n\\nLANGUAGE: python\\nCODE:\\n```\\nchain = prompt | llm | extract_json\\n\\nchain.invoke({\"query\": query})\\n```\\n\\n----------------------------------------\\n\\nTITLE: Import LocalAIEmbeddings for Langchain\\nDESCRIPTION: This Python import statement brings the `LocalAIEmbeddings` class into scope from the `langchain_localai` package. This class is essential for utilizing LocalAI\\'s text embedding capabilities within a Langchain application, allowing for local processing of embedding models.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/providers/localai\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom langchain_localai import LocalAIEmbeddings\\n```\\n\\n----------------------------------------\\n\\nTITLE: Select Ollama as Large Language Model\\nDESCRIPTION: Initializes the Ollama large language model for local processing, specifying \\'llama2\\' as the model to be used.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/vectorstores/aperturedb\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nfrom langchain_community.llms import Ollama\\n\\nllm = Ollama(model=\"llama2\")\\n```\\n\\n----------------------------------------\\n\\nTITLE: Chaining FMP Data Tools with LangChain Models\\nDESCRIPTION: Illustrates how to integrate FMP data tools into a LangChain expression language (LCEL) chain. It sets up an LLM, initializes the `FMPDataToolkit`, binds the tools to the LLM, and creates a chain to process a natural language query about stock analysis.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/tools/fmp-data\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_openai import ChatOpenAI\\n\\n# Setup\\nllm = ChatOpenAI(temperature=0)\\ntoolkit = FMPDataToolkit(query=\"Stock analysis\", num_results=3)\\ntools = toolkit.get_tools()\\n\\nllm_with_tools = llm.bind(functions=tools)\\noutput_parser = StrOutputParser()\\n# Create chain\\nrunner = llm_with_tools | output_parser\\n\\n# Run chain\\nresponse = runner.invoke(\\n    {\\n        \"input\": \"What\\'s the PE ratio of Microsoft?\"\\n    }\\n)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Configure Reasoning Output from OpenAI Models in LangChain\\nDESCRIPTION: Explains how to configure LangChain\\'s `ChatOpenAI` to return a summary of the model\\'s reasoning process by setting the `reasoning` parameter. This feature automatically routes to the Responses API and allows for inspecting the model\\'s thought process.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/chat/openai\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom langchain_openai import ChatOpenAI\\n\\nreasoning = {\\n    \"effort\": \"medium\",  # \\'low\\', \\'medium\\', or \\'high\\'\\n    \"summary\": \"auto\",  # \\'detailed\\', \\'auto\\', or None\\n}\\n\\nllm = ChatOpenAI(model=\"o4-mini\", reasoning=reasoning)\\nresponse = llm.invoke(\"What is 3^3?\")\\n\\n# Output\\nresponse.text()\\n```\\n\\nLANGUAGE: python\\nCODE:\\n```\\n# Reasoning\\nreasoning = response.additional_kwargs[\"reasoning\"]\\nfor block in reasoning[\"summary\"]:\\n    print(block[\"text\"])\\n```\\n\\n----------------------------------------\\n\\nTITLE: Prompt Baseten Model for Single Call\\nDESCRIPTION: Demonstrates a basic single call to the loaded Baseten Mistral model. The model processes the input prompt and returns a response.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/llms/baseten\\n\\nLANGUAGE: python\\nCODE:\\n```\\n# Prompt the model\\nmistral(\"What is the Mistral wind?\")\\n```\\n\\n----------------------------------------\\n\\nTITLE: Load MLX Model from ID using MLXPipeline\\nDESCRIPTION: Demonstrates how to load a quantized MLX model from a Hugging Face model ID using the `MLXPipeline.from_model_id` method, specifying pipeline arguments like `max_tokens` and `temp`.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/llms/mlx_pipelines\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom langchain_community.llms.mlx_pipeline import MLXPipeline\\n\\npipe = MLXPipeline.from_model_id(\\n    \"mlx-community/quantized-gemma-2b-it\",\\n    pipeline_kwargs={\"max_tokens\": 10, \"temp\": 0.1},\\n)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Batch Process Inputs with ChatParrotLink\\nDESCRIPTION: Illustrates how to use the `batch` method to process multiple string inputs simultaneously with the `ChatParrotLink` model. The output is a list of `AIMessage` responses, one for each input.\\nSOURCE: https://python.langchain.com/docs/introduction/how_to/custom_chat_model\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nmodel.batch([\"hello\", \"goodbye\"])\\n```\\n\\nLANGUAGE: Python\\nCODE:\\n```\\n[AIMessage(content=\\'hel\\', additional_kwargs={}, response_metadata={\\'time_in_seconds\\': 3}, id=\\'run-eea4ed7d-d750-48dc-90c0-7acca1ff388f-0\\', usage_metadata={\\'input_tokens\\': 5, \\'output_tokens\\': 3, \\'total_tokens\\': 8}),\\n AIMessage(content=\\'goo\\', additional_kwargs={}, response_metadata={\\'time_in_seconds\\': 3}, id=\\'run-07cfc5c1-3c62-485f-b1e0-3d46e1547287-0\\', usage_metadata={\\'input_tokens\\': 7, \\'output_tokens\\': 3, \\'total_tokens\\': 10})]\\n```\\n\\n----------------------------------------\\n\\nTITLE: Get Available Models with NVIDIA LangChain\\nDESCRIPTION: Demonstrates how to query the available models using the NVIDIA integration in LangChain. It shows both the direct NVIDIA call and a commented-out generic LLM call for comparison.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/llms/nvidia_ai_endpoints\\n\\nLANGUAGE: python\\nCODE:\\n```\\nNVIDIA.get_available_models()\\n# llm.get_available_models()\\n```\\n\\n----------------------------------------\\n\\nTITLE: Load IPEX-LLM Low-bit Model from Path\\nDESCRIPTION: Explains how to reload a previously saved low-bit model using `IpexLLM.from_model_id_low_bit`. This method is faster and more memory-efficient than `from_model_id` as it skips the conversion step. Note that tokenizers must be handled separately.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/llms/ipex_llm\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nllm_lowbit = IpexLLM.from_model_id_low_bit(\\n    model_id=saved_lowbit_model_path,\\n    tokenizer_id=\"lmsys/vicuna-7b-v1.5\",\\n    # tokenizer_name=saved_lowbit_model_path,  # copy the tokenizers to saved path if you want to use it this way\\n    model_kwargs={\"temperature\": 0, \"max_length\": 64, \"trust_remote_code\": True},\\n)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Invoke ChatWriter Model with Bound Tools\\nDESCRIPTION: Shows how to invoke the ChatWriter model after tools have been bound, allowing the model to utilize the defined tools based on the input prompt.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/chat/writer\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nai_msg = llm.invoke(\\n    \"what is the weather like in New York City\",\\n)\\nai_msg\\n```\\n\\n----------------------------------------\\n\\nTITLE: Invoke LLM with Human Message to Get Tool Calls\\nDESCRIPTION: This code constructs a `HumanMessage` with a query that prompts the model to use its bound tools. It then invokes the language model with this message, expecting the model to return `tool_calls` based on the query. The `ai_msg.tool_calls` are printed and appended to the message history.\\nSOURCE: https://python.langchain.com/docs/introduction/how_to/tool_results_pass_to_model\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom langchain_core.messages import HumanMessage\\n\\nquery = \"What is 3 * 12? Also, what is 11 + 49?\"\\n\\nmessages = [HumanMessage(query)]\\n\\nai_msg = llm_with_tools.invoke(messages)\\n\\nprint(ai_msg.tool_calls)\\n\\nmessages.append(ai_msg)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Filter Redundant Documents from Merged Retrievers using EmbeddingsRedundantFilter\\nDESCRIPTION: This code illustrates how to remove redundant results from the documents retrieved by the `MergerRetriever`. It utilizes an `EmbeddingsRedundantFilter` with a separate embedding model to identify and filter out similar documents. The filter is then integrated into a `DocumentCompressorPipeline`, which is subsequently used by a `ContextualCompressionRetriever` to process the output of the `MergerRetriever`.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/retrievers/merger_retriever\\n\\nLANGUAGE: python\\nCODE:\\n```\\n# We can remove redundant results from both retrievers using yet another embedding.\\n# Using multiples embeddings in diff steps could help reduce biases.\\nfilter = EmbeddingsRedundantFilter(embeddings=filter_embeddings)\\npipeline = DocumentCompressorPipeline(transformers=[filter])\\ncompression_retriever = ContextualCompressionRetriever(\\n    base_compressor=pipeline, base_retriever=lotr\\n)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Invoke LLMChain with Loaded Low-bit IPEX-LLM Model\\nDESCRIPTION: Demonstrates using the reloaded low-bit IPEX-LLM model within a LangChain chain. This snippet confirms that the model loaded from a saved path can be seamlessly integrated and used for inference.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/llms/ipex_llm\\n\\nLANGUAGE: Python\\nCODE:\\n```\\nllm_chain = prompt | llm_lowbit\\n\\nquestion = \"What is AI?\"\\noutput = llm_chain.invoke(question)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Perform Question Answering with LangChain and OpenAI\\nDESCRIPTION: This Python code illustrates how to initialize and use a question-answering chain from LangChain with an OpenAI language model. It utilizes `load_qa_chain` with a \\'map_reduce\\' chain type to process input documents and answer a given query. Note that `documents` is an assumed input variable not defined in this snippet.\\nSOURCE: https://python.langchain.com/docs/introduction/integrations/document_loaders/amazon_textract\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom langchain.chains.question_answering import load_qa_chain\\nfrom langchain_openai import OpenAI\\n\\nchain = load_qa_chain(llm=OpenAI(), chain_type=\"map_reduce\")\\nquery = [\"Who are the autors?\"]\\n\\nchain.run(input_documents=documents, question=query)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Initialize OpenAI Chat Model\\nDESCRIPTION: Initializes an instance of `ChatOpenAI` using the \\'gpt-3.5-turbo\\' model. This model instance will be used to process user inputs and potentially invoke functions.\\nSOURCE: https://python.langchain.com/docs/introduction/how_to/tools_as_openai_functions\\n\\nLANGUAGE: python\\nCODE:\\n```\\nmodel = ChatOpenAI(model=\"gpt-3.5-turbo\")\\n```', name='scrap_docs', tool_call_id='call_JDYofLPq4oCpcfW1UupJelJ9')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c473e8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a FastAPI application that wraps the provided code with a POST endpoint for executing LLM queries and system prompts. This will allow you to send requests from Postman.\n",
      "\n",
      "```python\n",
      "from fastapi import FastAPI, HTTPException\n",
      "from pydantic import BaseModel\n",
      "from langchain.chat_models import init_chat_model\n",
      "from langchain_core.messages import SystemMessage, HumanMessage\n",
      "from some_module import scrap_docs  # make sure to import your scrap_docs function correctly\n",
      "\n",
      "# Initialize the chat model\n",
      "llm = init_chat_model(\"gpt-4.1\", model_provider=\"openai\")\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "# Define request model\n",
      "class QueryRequest(BaseModel):\n",
      "    query: str\n",
      "    system_prompt: str\n",
      "\n",
      "@app.post(\"/execute\")\n",
      "async def execute_query(request: QueryRequest):\n",
      "    query = request.query\n",
      "    system_prompt = request.system_prompt\n",
      "\n",
      "    try:\n",
      "        def execute_llm(llm, query: str, system_promt: str):\n",
      "            tools_registry = {\n",
      "                \"scrap_docs\": scrap_docs\n",
      "            }\n",
      "    \n",
      "            tools = [scrap_docs]\n",
      "            llm_with_tools = llm.bind_tools(tools)\n",
      "    \n",
      "            messages = [\n",
      "                SystemMessage(system_promt),\n",
      "                HumanMessage(query),\n",
      "            ]\n",
      "    \n",
      "            useful_libs = lib_extractor_llm.invoke(query)\n",
      "            useful_libs = useful_libs.to_dict()\n",
      "    \n",
      "            libs_text = \"\"\n",
      "    \n",
      "            for k, v in useful_libs.items():\n",
      "                libs_text += f\"{k} search for `{v}`\\n\"\n",
      "    \n",
      "            next_prompt = f\"get the docs and search for the topics of the following libraries\\n{libs_text}\"\n",
      "    \n",
      "            ai_message = llm_with_tools.invoke(next_prompt)\n",
      "            messages.append(ai_message)\n",
      "    \n",
      "            tool_calls = ai_message.tool_calls\n",
      "    \n",
      "            for tool_call in tool_calls:\n",
      "                selected_tool = tools_registry[tool_call['name']]\n",
      "                tool_msg = selected_tool.invoke(tool_call)\n",
      "                messages.append(tool_msg)\n",
      "    \n",
      "            output = llm_with_tools.invoke(messages)\n",
      "            return (output, messages)\n",
      "        \n",
      "        result, full_messages = execute_llm(llm, query, system_prompt)\n",
      "        return {\"result\": result, \"full_messages\": full_messages}\n",
      "    \n",
      "    except Exception as e:\n",
      "        raise HTTPException(status_code=500, detail=str(e))\n",
      "\n",
      "# Add this line if you need to run the server using command: uvicorn <filename>:app --reload\n",
      "if __name__ == \"__main__\":\n",
      "    import uvicorn\n",
      "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
      "```\n",
      "\n",
      "Replace `some_module` with the actual module name where your `scrap_docs` function is defined. You would also need to define the `lib_extractor_llm.invoke()` method or replace it with the relevant logic.\n"
     ]
    }
   ],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632e76ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from helpers import scrap_docs  # make sure to import your scrap_docs function correctly\n",
    "\n",
    "# Initialize the chat model\n",
    "llm = init_chat_model(\"gpt-4.1\", model_provider=\"openai\")\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Define request model\n",
    "class QueryRequest(BaseModel):\n",
    "    query: str\n",
    "    system_prompt: str\n",
    "\n",
    "@app.post(\"/execute\")\n",
    "async def execute_query(request: QueryRequest):\n",
    "    query = request.query\n",
    "    system_prompt = request.system_prompt\n",
    "\n",
    "    try:\n",
    "        def execute_llm(llm, query: str, system_promt: str):\n",
    "            tools_registry = {\n",
    "                \"scrap_docs\": scrap_docs\n",
    "            }\n",
    "    \n",
    "            tools = [scrap_docs]\n",
    "            llm_with_tools = llm.bind_tools(tools)\n",
    "    \n",
    "            messages = [\n",
    "                SystemMessage(system_promt),\n",
    "                HumanMessage(query),\n",
    "            ]\n",
    "    \n",
    "            useful_libs = lib_extractor_llm.invoke(query)\n",
    "            useful_libs = useful_libs.to_dict()\n",
    "    \n",
    "            libs_text = \"\"\n",
    "    \n",
    "            for k, v in useful_libs.items():\n",
    "                libs_text += f\"{k} search for `{v}`\\n\"\n",
    "    \n",
    "            next_prompt = f\"get the docs and search for the topics of the following libraries\\n{libs_text}\"\n",
    "    \n",
    "            ai_message = llm_with_tools.invoke(next_prompt)\n",
    "            messages.append(ai_message)\n",
    "    \n",
    "            tool_calls = ai_message.tool_calls\n",
    "    \n",
    "            for tool_call in tool_calls:\n",
    "                selected_tool = tools_registry[tool_call['name']]\n",
    "                tool_msg = selected_tool.invoke(tool_call)\n",
    "                messages.append(tool_msg)\n",
    "    \n",
    "            output = llm_with_tools.invoke(messages)\n",
    "            return (output, messages)\n",
    "        \n",
    "        result, full_messages = execute_llm(llm, query, system_prompt)\n",
    "        return {\"result\": result, \"full_messages\": full_messages}\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Add this line if you need to run the server using command: uvicorn <filename>:app --reload\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
