{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "849d6f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index `first-index` already exists\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pinecone_key = \"pcsk_4PR9Ek_TvzdWZi5dsDCvKCZStfatGWfV5Mog5hbS6ccmZGd4W5bA6Q5EAygQ1LQRVVxn83\"\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_key)\n",
    "\n",
    "index_name = \"first-index\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index_for_model(\n",
    "        name=index_name,\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\",\n",
    "        embed={\n",
    "            \"model\":\"llama-text-embed-v2\",\n",
    "            \"field_map\":{\"text\": \"chunk_text\"}\n",
    "        }\n",
    "    )\n",
    "else:\n",
    "    print(f\"Index `{index_name}` already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25f1e624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import parse_snippet\n",
    "from helpers import get_docs\n",
    "from helpers import pretty_print_dict\n",
    "\n",
    "# url = \"https://context7.com/context7/python_langchain_com-docs-introduction/llms.txt?tokens=10000\"\n",
    "url = \"https://context7.com/context7/python_langchain_com-docs-introduction/llms.txt?tokens=100000\"\n",
    "docs = get_docs(url)\n",
    "parsed_docs = list(map(lambda x: parse_snippet(x), docs))\n",
    "\n",
    "parsed_docs = parsed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "440a3ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TITLE\": \"Define and Use RunnableLambda with TypedDict Input\",\n",
      "    \"DESCRIPTION\": \"Demonstrates how to create a RunnableLambda from a function that accepts a TypedDict as input. It shows how to convert this runnable into a tool using as_tool, access its description (which is 'Explanation of when to use tool.') and its automatically generated schema (e.g., {'title': 'My tool', 'type': 'object', 'properties': {'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'array', 'items': {'type': 'integer'}}}, 'required': ['a', 'b']}). The tool is then invoked with sample data, producing the output '6'.\",\n",
      "    \"SOURCE\": \"https://python.langchain.com/docs/introduction/how_to/convert_runnable_to_tool\",\n",
      "    \"LANGUAGE\": \"python\",\n",
      "    \"CODE\": \"from typing import List\\n\\nfrom langchain_core.runnables import RunnableLambda\\nfrom typing_extensions import TypedDict\\n\\nclass Args(TypedDict):\\n    a: int\\n    b: List[int]\\n\\ndef f(x: Args) -> str:\\n    return str(x[\\\"a\\\"] * max(x[\\\"b\\\"]))\\n\\nrunnable = RunnableLambda(f)\\nas_tool = runnable.as_tool(\\n    name=\\\"My tool\\\",\\n    description=\\\"Explanation of when to use tool.\\\",\\n)\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "len(docs)\n",
    "pretty_print_dict(parsed_docs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f93a426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07fc36c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1024,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'cosine',\n",
       " 'namespaces': {'ns1': {'vector_count': 10}},\n",
       " 'total_vector_count': 10,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "788e28c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "parsed_docs_with_ids = [\n",
    "    {\"_id\": str(uuid.uuid4()), **{(\"text\" if k == \"TITLE\" else k): v for k, v in doc.items()}}\n",
    "    for doc in parsed_docs\n",
    "]\n",
    "\n",
    "# parsed_docs_with_ids[1]\n",
    "\n",
    "index.upsert_records(\"langchain\", parsed_docs_with_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e717417",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [\n",
    "    {\"_id\": \"r1\", \"text\": \"# to create a numpy array use: np.array([1, 2, 3])\", \"category\": \"programming\"}\n",
    "]\n",
    "\n",
    "index.upsert_records(\"ns1\", records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72e96916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': 'a2b33d0a-16b5-4e2c-ad96-96bafe1f7fdb',\n",
      " '_score': 0.31182077527046204,\n",
      " 'fields': {'CODE': 'from langchain.chains import '\n",
      "                    'create_history_aware_retriever, create_retrieval_chain\\n'\n",
      "                    'from langchain.chains.combine_documents import '\n",
      "                    'create_stuff_documents_chain\\n'\n",
      "                    '\\n'\n",
      "                    'condense_question_system_template = (\\n'\n",
      "                    '    \"Given a chat history and the latest user question \"\\n'\n",
      "                    '    \"which might reference context in the chat history, '\n",
      "                    '\"\\n'\n",
      "                    '    \"formulate a standalone question which can be '\n",
      "                    'understood \"\\n'\n",
      "                    '    \"without the chat history. Do NOT answer the '\n",
      "                    'question, \"\\n'\n",
      "                    '    \"just reformulate it if needed and otherwise return '\n",
      "                    'it as is.\"\\n'\n",
      "                    ')\\n'\n",
      "                    '\\n'\n",
      "                    'condense_question_prompt = '\n",
      "                    'ChatPromptTemplate.from_messages(\\n'\n",
      "                    '    [\\n'\n",
      "                    '        (\"system\", condense_question_system_template),\\n'\n",
      "                    '        (\"placeholder\", \"{chat_history}\"),\\n'\n",
      "                    '        (\"human\", \"{input}\"),\\n'\n",
      "                    '    ]\\n'\n",
      "                    ')\\n'\n",
      "                    'history_aware_retriever = '\n",
      "                    'create_history_aware_retriever(\\n'\n",
      "                    '    llm, vectorstore.as_retriever(), '\n",
      "                    'condense_question_prompt\\n'\n",
      "                    ')\\n'\n",
      "                    '\\n'\n",
      "                    'system_prompt = (\\n'\n",
      "                    '    \"You are an assistant for question-answering tasks. '\n",
      "                    '\"\\n'\n",
      "                    '    \"Use the following pieces of retrieved context to '\n",
      "                    'answer \"\\n'\n",
      "                    '    \"the question. If you don\\'t know the answer, say '\n",
      "                    'that you \"\\n'\n",
      "                    '    \"don\\'t know. Use three sentences maximum and keep '\n",
      "                    'the \"\\n'\n",
      "                    '    \"answer concise.\"\\n'\n",
      "                    '    \"\\\\n\\\\n\"\\n'\n",
      "                    '    \"{context}\"\\n'\n",
      "                    ')\\n'\n",
      "                    '\\n'\n",
      "                    'qa_prompt = ChatPromptTemplate.from_messages(\\n'\n",
      "                    '    [\\n'\n",
      "                    '        (\"system\", system_prompt),\\n'\n",
      "                    '        (\"placeholder\", \"{chat_history}\"),\\n'\n",
      "                    '        (\"human\", \"{input}\"),\\n'\n",
      "                    '    ]\\n'\n",
      "                    ')\\n'\n",
      "                    'qa_chain = create_stuff_documents_chain(llm, qa_prompt)\\n'\n",
      "                    '\\n'\n",
      "                    'convo_qa_chain = '\n",
      "                    'create_retrieval_chain(history_aware_retriever, '\n",
      "                    'qa_chain)\\n'\n",
      "                    '\\n'\n",
      "                    'convo_qa_chain.invoke(\\n'\n",
      "                    '    {\\n'\n",
      "                    '        \"input\": \"What are autonomous agents?\",\\n'\n",
      "                    '        \"chat_history\": [],\\n'\n",
      "                    '    }\\n'\n",
      "                    ')\\n',\n",
      "            'DESCRIPTION': 'This Python snippet demonstrates how to construct '\n",
      "                           \"a conversational retrieval chain using LangChain's \"\n",
      "                           'LCEL. It sets up a history-aware retriever to '\n",
      "                           'reformulate questions based on chat history and a '\n",
      "                           'document chain to answer questions using retrieved '\n",
      "                           'context, finally combining them into a full '\n",
      "                           'conversational QA chain. The example includes '\n",
      "                           'prompt templates for both question condensation '\n",
      "                           'and answer generation.',\n",
      "            'LANGUAGE': 'python',\n",
      "            'SOURCE': 'https://python.langchain.com/docs/introduction/versions/migrating_chains/conversation_retrieval_chain',\n",
      "            'text': 'Build Conversational Retrieval Chain with LangChain '\n",
      "                    'Expression Language (LCEL)'}}, {'_id': '22881250-e913-430d-8fda-2f7785e1bbc4',\n",
      " '_score': 0.24629928171634674,\n",
      " 'fields': {'CODE': 'llm.invoke(\"Hello how are you?\")\\n',\n",
      "            'DESCRIPTION': 'This Python snippet demonstrates how to call the '\n",
      "                           'instantiated llm object with a simple prompt. The '\n",
      "                           'invoke method sends the input string to the OpenAI '\n",
      "                           'model and returns the generated text completion.',\n",
      "            'LANGUAGE': 'python',\n",
      "            'SOURCE': 'https://python.langchain.com/docs/introduction/integrations/llms/openai',\n",
      "            'text': 'Invoke OpenAI Language Model for Text Completion'}}]\n"
     ]
    }
   ],
   "source": [
    "query = \"Conversational bot\"\n",
    "\n",
    "results = index.search(\n",
    "    namespace=\"langchain\",\n",
    "    query={\n",
    "        \"top_k\": 2,\n",
    "        \"inputs\": {\n",
    "            'text': query\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(results['result']['hits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1758db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': {'hits': [{'_id': 'rec1',\n",
       "                      '_score': 0.10650458931922913,\n",
       "                      'fields': {'category': 'history',\n",
       "                                 'text': 'The Eiffel Tower was completed in '\n",
       "                                         '1889 and stands in Paris, France.'}},\n",
       "                     {'_id': 'rec7',\n",
       "                      '_score': 0.06278920918703079,\n",
       "                      'fields': {'category': 'history',\n",
       "                                 'text': 'The Great Wall of China was built to '\n",
       "                                         'protect against invasions.'}},\n",
       "                     {'_id': 'rec5',\n",
       "                      '_score': 3.21923362207599e-05,\n",
       "                      'fields': {'category': 'literature',\n",
       "                                 'text': 'Shakespeare wrote many famous plays, '\n",
       "                                         'including Hamlet and Macbeth.'}},\n",
       "                     {'_id': 'rec4',\n",
       "                      '_score': 1.6187581422855146e-05,\n",
       "                      'fields': {'category': 'biology',\n",
       "                                 'text': 'The mitochondrion is often called '\n",
       "                                         'the powerhouse of the cell.'}},\n",
       "                     {'_id': 'rec3',\n",
       "                      '_score': 1.5936620911816135e-05,\n",
       "                      'fields': {'category': 'science',\n",
       "                                 'text': 'Albert Einstein developed the theory '\n",
       "                                         'of relativity.'}}]},\n",
       " 'usage': {'embed_total_tokens': 8, 'read_units': 1, 'rerank_units': 1}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranked_results = index.search(\n",
    "    namespace=\"ns1\",\n",
    "    query={\n",
    "        \"top_k\": 5,\n",
    "        \"inputs\": {\n",
    "            'text': query\n",
    "        }\n",
    "    },\n",
    "    rerank={\n",
    "        \"model\": \"bge-reranker-v2-m3\",\n",
    "        \"top_n\": 5,\n",
    "        \"rank_fields\": [\"text\"]\n",
    "    },\n",
    "    fields=[\"category\", \"text\"]\n",
    ")\n",
    "\n",
    "reranked_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
